---
title: "draft_larry"
author: "Linxuan Wang, Jingan Zhou"
date: "2022-12-03"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,cache=T)
library(tidyverse)
library(ggplot2)
library(GGally)
library(pROC)
library(patchwork)
library(stats)
library(tidymodels)
```

```{r, test-train splitting}
set.seed(1223)
# split testing - 20%
image.valid <- rbind(image1.valid,image2.valid,image3.valid)
coordinate = image.valid %>% dplyr::select(x, y)

class.vsplit = folds.Vsplit(coordinate, 5)

# the fourth one as testing
test.vsplit = image.valid[which(class.vsplit == 4), ]
# train
train.vsplit = image.valid[which(class.vsplit != 4), ]

# prepare features, labels and coordinates
features = train.vsplit[,4:11]
labels = train.vsplit[,3]
coordinate = train.vsplit[,1:2]
```

```{r}
loss.mean = function(label, predicted){
  return (mean(label != predicted))
}
```

```{r, random forest}

CVmaster = function(classifier, features, labels, K = 10, loss_method, split_method, coordinate, tree.num = seq(50,600,50), tree.depth = seq(3,20,1)){
  # get K folds
  fold = split_method(coordinate, K)
  
  # record loss
  loss = c()
  
  # record loss for random forest, not used under other classifiers
  loss.rf <- array(rep(1, length(tree.num)*length(tree.depth)*K), dim=c(length(tree.num),length(tree.depth),K))
  
  for(k in 1:K){
    # separate into training and validation
    position = which(fold == k)
    
    # features
    features.train = features[-position,]
    features.validate = features[position,]
    
    # labels
    labels.train = labels[-position]
    labels.validate = labels[position]
    
    # data for random forest training
    data.train <- cbind(features.train,label=labels.train)
    data.val <- cbind(features.validate,label=labels.validate)
    
    if (classifier == "randomForest"){ #should be else if
      # parameter tuning
      for (i in 1:length(tree.num)){
        for (j in 1:length(tree.depth)){
          # training
          fit <- ranger(label~NDAI+SD+CORR+DF+CF+BF+AF+AN,
                        data=data.train,
                        num.trees = tree.num[i],
                        max.depth = tree.depth[j], 
                        num.threads = parallel::detectCores()-1,
                        classification = TRUE,
                        seed=1223)
          
          # predicting
          pred <- predict(fit,data=data.val)
          
          #compute loss
          loss.rf[i,j,k] <- loss_method(data.val$label, pred$predictions)
       }
     }
    }
    
    else if (classifier == "Logistic"){
      # label = -1 --> label = 0
      data.train$label[data.train$label == -1] = 0
      data.val$label[data.val$label == -1] = 0
      fit <- glm(label~NDAI+SD+CORR+DF+CF+BF+AF+AN,family = "binomial",data = data.train)
      predict_logistic <- predict(fit,data.val,type = "response")
      
      predict_logistic <- ifelse(predict_logistic > 0.5, 1, 0)
      
      loss <- c(loss,loss_method(predict_logistic, data.val$label))
    }
  }
  
  # should be put outside of the "for each k" part in draft_andy.Rmd
  if (classifier == "randomForest"){
    loss.rf <- apply(loss.rf,c(1,2),mean)

    # choose the best parameter
    i=which(loss.rf == min(loss.rf),arr.ind = TRUE)[1]
    j=which(loss.rf == min(loss.rf),arr.ind = TRUE)[2]
    
    # cv under tuned parameter
    loss = c()
    for(k in 1:K){
      # separate into training and validation
      position = which(fold == k)
      
      # features
      features.train = features[-position,]
      features.validate = features[position,]
      
      # labels
      labels.train = labels[-position]
      labels.validate = labels[position]
      
      # data for random forest training
      data.train <- cbind(features.train,label=labels.train)
      data.val <- cbind(features.validate,label=labels.validate)
      
      # training
      fit <- ranger(label~NDAI+SD+CORR+DF+CF+BF+AF+AN,
                    data=data.train,
                    num.trees = tree.num[i],
                    max.depth = tree.depth[j], 
                    num.threads = parallel::detectCores()-1,
                    classification = TRUE)
      
      # predicting
      pred <- predict(fit,data=data.val)
      
      # compute loss
      loss = c(loss, loss_method(data.val$label, pred$predictions))
    }
    return(list(cv_loss=loss,tree_num=tree.num[i],tree_depth=tree.depth[j]))
  }
}

```

```{r}
# testing random forest
results.rf <- CVmaster(classifier = "randomForest", 
                       features, 
                       labels, 
                       K = 10, 
                       loss_method = loss.mean, 
                       split_method = folds.Kmean, 
                       coordinate, 
                       tree.num = seq(300,350,50), 
                       tree.depth = seq(5,6,1))

# probablistic prediction, need cutoff values to complete classification
fit.rf <- ranger(label~NDAI+SD+CORR+DF+CF+BF+AF+AN,
            data=train.vsplit,
            num.trees =results.rf$tree_num,
            max.depth = results.rf$tree_depth,
            num.threads = parallel::detectCores()-1,
            classification = TRUE,
            probability = TRUE,
            seed=1223)


# store predicted probabilities of labels equal to 1
prob.pred.rf <- predict(fit.rf,data=test.vsplit)$predictions[,1]

# ROC curve
# the threshold with highest sensitivity+specificity is printed in the plot
plot.roc(test.vsplit$label,prob.pred.rf,percent=TRUE,thresholds='best',print.thres="best",legacy.axes = T,auc.polygon = T) 
 
```

```{r}
# testing logistic
test.vsplit$label[test.vsplit$label == -1] = 0
fit.logistic <- glm(label~NDAI+SD+CORR+DF+CF+BF+AF+AN,family = "binomial",data = test.vsplit)
prob.pred.logis <- predict(fit.logistic,test.vsplit,type = "response")

plot.roc(test.vsplit$label,prob.pred.logis,percent=TRUE,thresholds='best',print.thres="best",legacy.axes = T,auc.polygon = T,add=T)
```

```{r}
# ROC curves
roc.obj.rf <- roc(test.vsplit$label, prob.pred.rf)
roc.obj.logis <- roc(test.vsplit$label, prob.pred.logis)

coords(roc.obj.logis, "best", ret=c("threshold", "specificity", "sensitivity"))
coords(roc.obj.rf, "best", ret=c("threshold", "specificity", "sensitivity"))
  

ggroc(list(logistic=roc.obj.logis,randomForest=roc.obj.rf)) +
  geom_point(aes(x=0.9188396, y=0.9847082), colour="red") +
  geom_point(aes(x=0.9107317, y=0.9719358), colour="blue") +
  annotate("text", x=0.9188396, y=1, label = "0.3564",size=3,color='red') +
  annotate("text", x=0.87, y=0.9719358, label = "0.3011",size=3,color='blue') +
  labs(fill = "Dose (mg)")


# another way to make roc curve
# plot(roc.obj.rf,
#      print.auc = TRUE,
#      auc.polygon = TRUE,
#      max.auc.polygon = TRUE,
#      legacy.axes = TRUE,
#      auc.polygon.col = "lightblue",
#       print.thres = "best",
#      print.auc.x = 0.4,
#      print.auc.y = 0.2
#      )
# 
# plot(roc.obj.logis,
#      print.auc = TRUE,
#      auc.polygon = TRUE,
#      max.auc.polygon = TRUE,
#      legacy.axes = TRUE,
#      auc.polygon.col = "lightyellow",
#       print.thres = "best",
#      print.auc.x = 0.4,
#      print.auc.y = 0.2,
#      add=TRUE
#      )

```


