## Question 2

### (a)
Ideally, we want our training and validation sets to be as different as possible since the real pictures can be really heterogeneous. If we just use random number generator to specify the samples that should be thrown to the training and validation set respectively, the training "image" and validation "image" will intersect with each other severely. High degree of intersection leads to similarity between training and validation images because samples that are near to each other tend to have same labels. In that case, the validation error is not a good proxy for testing error, and thus testing error will possibly be much higher than validation error when we apply the classification model on a new image.

To take the fact that data is not i.i.d. into account, we would like to split the image into several "blocks" that are treated as sub-images, such that each block only borders other blocks at the boundary. This data splitting rule significantly reduces the similarity between training set and validation set as well as the testing set. In the meantime, we want each sub-image contains fair amount of pixels of clouds and surfaces to achieve better performances. Here, we introduce two approaches to create the blocks.

The first one is inspired by the nature of K-means clustering. 

```{r, separate by kmeans}
set.seed(1223)
km.out1.bad <- kmeans(image1.valid%>%.[c(4,5,6)],5) # bad result
km.out1 <- kmeans(image1.valid%>%.[c(1,2)],5) # good result, hence we use x,y label to split the data
km.out2 <- kmeans(image2.valid[c(1,2)],5)
km.out3 <- kmeans(image3.valid[c(1,2)],5)
```

```{r, visualize kmeans result}
# bad data split
image1.valid %>%
  mutate(class=km.out1.bad$cluster) %>%
  ggplot(aes(x,y)) +
  geom_raster(aes(fill=class),alpha=0.8) +
  scale_fill_gradientn(colours = c('red','green','blue','yellow','purple'))

# good data split
p21 <- image1.valid %>%
  mutate(class=km.out1$cluster) %>%
  ggplot(aes(x,y)) +
  geom_raster(aes(fill=class),alpha=0.8) +
  scale_fill_gradientn(colours = c('red','green','blue','yellow','purple'))

p22 <- image2.valid %>%
  mutate(class=km.out2$cluster) %>%
  ggplot(aes(x,y)) +
  geom_raster(aes(fill=class),alpha=0.8) +
  scale_fill_gradientn(colours = c('red','green','blue','yellow','purple'))

p23 <- image3.valid %>%
  mutate(class=km.out3$cluster) %>%
  ggplot(aes(x,y)) +
  geom_raster(aes(fill=class),alpha=0.8) +
  scale_fill_gradientn(colours = c('red','green','blue','yellow','purple'))

p21+p22+p23
```

Remember, we want each sub-image contains fair amount of pixels of clouds and surfaces.

3 images * 5 classes/image = 15 classes. We first randomly split train-val-test at a 9-3-3 ratio.

Some statistics: 200000 approx. valid data: 80000 with label 1 and 120000 with label -1. 
Ideally, 120000 training: 50000 with 1 and 70000 with label -1
          40000 val: 16000 with label 1 and 2400 with label -1
          40000 test: 16000 with label 1 and 2400 with label -1
          
```{r, combine images with kmean classes}
rbind(image1.valid %>%
  mutate(class=km.out1$cluster),
  image2.valid %>%
  mutate(class=km.out2$cluster),
  image3.valid %>%
  mutate(class=km.out3$cluster)) %>%
  group_by(image,class,label) %>%
  summarise(n=n())
```

Validation: (1,3), (2,1), (2,5) -1/1=28514/14740
Testing: (1,1), (2,3), (3,5) -1/1=26568/15676
Training: 71998/50565

```{r, dividing into training/testing/validation sets}
image.val1 <- rbind(image1.valid %>%
  mutate(class=km.out1$cluster)%>%filter(class==3),
  image2.valid %>%
  mutate(class=km.out2$cluster)%>%filter(class %in% c(1,5)))

image.test1 <- rbind(image1.valid %>%
  mutate(class=km.out1$cluster)%>%filter(class==1),
  image2.valid %>%
  mutate(class=km.out2$cluster)%>%filter(class==3),
  image3.valid %>%
  mutate(class=km.out3$cluster)%>%filter(class==5))

image.train1 <- rbind(image1.valid %>%
  mutate(class=km.out1$cluster)%>%filter(!(class %in% c(1,3))),
  image2.valid %>%
  mutate(class=km.out2$cluster)%>%filter(!(class %in% c(1,3,5))),
  image3.valid %>%
  mutate(class=km.out3$cluster)%>%filter(class!=5))
```

The second one: vertical split. For each image, we simply divide the image into 5 sub-images vertically, each of which contains one-fifth pixels of the whole image. Then we look into all the 15 sub-images and assign them into training, validation and testing set.

```{r, separate by vsplit}
quantile1 <- round(as.numeric(quantile(seq_len(nrow(image1.valid)),c(0,0.2,0.4,0.6,0.8,1))))
quantile2 <- round(as.numeric(quantile(seq_len(nrow(image2.valid)),c(0,0.2,0.4,0.6,0.8,1))))
quantile3 <- round(as.numeric(quantile(seq_len(nrow(image3.valid)),c(0,0.2,0.4,0.6,0.8,1))))

label1 <- c(rep(1,quantile1[2]-quantile1[1]),
            rep(2,quantile1[3]-quantile1[2]),
            rep(3,quantile1[4]-quantile1[3]),
            rep(4,quantile1[5]-quantile1[4]),
            rep(5,quantile1[6]-quantile1[5]+1))
label2 <- c(rep(1,quantile2[2]-quantile2[1]),
            rep(2,quantile2[3]-quantile2[2]),
            rep(3,quantile2[4]-quantile2[3]),
            rep(4,quantile2[5]-quantile2[4]),
            rep(5,quantile2[6]-quantile2[5]+1))
label3 <- c(rep(1,quantile3[2]-quantile3[1]),
            rep(2,quantile3[3]-quantile3[2]),
            rep(3,quantile3[4]-quantile3[3]),
            rep(4,quantile3[5]-quantile3[4]),
            rep(5,quantile3[6]-quantile3[5]+1))
```

```{r, combine images with vsplit indicators}
rbind(image1.valid %>%
  mutate(class=label1),
  image2.valid %>%
  mutate(class=label2),
  image3.valid %>%
  mutate(class=label3)) %>%
  group_by(image,class,label) %>%
  summarise(n=n())

rbind(image1.valid %>%
  mutate(class=label1),
  image2.valid %>%
  mutate(class=label2),
  image3.valid %>%
  mutate(class=label3)) %>%
  group_by(label) %>%
  summarise(n=n())
```

Validation: (1,2), (2,2), (2,5) -1/1=25214/19583
Testing: (1,1), (2,3), (3,4) -1/1=24684/16928
Training: 77182/44470

```{r, dividing into training/testing/validation sets}
image.val2 <- rbind(image1.valid %>%
  mutate(class=label1)%>%filter(class==2),
  image2.valid %>%
  mutate(class=label2)%>%filter(class %in% c(2,5)))

image.test2 <- rbind(image1.valid %>%
  mutate(class=label1)%>%filter(class==1),
  image2.valid %>%
  mutate(class=label2)%>%filter(class==3),
  image3.valid %>%
  mutate(class=label3)%>%filter(class==4))

image.train2 <- rbind(image1.valid %>%
  mutate(class=label1)%>%filter(!(class %in% c(1,2))),
  image2.valid %>%
  mutate(class=label2)%>%filter(!(class %in% c(2,3,5))),
  image3.valid %>%
  mutate(class=label3)%>%filter(class!=4))
```

### (b)

```{r, benchmark values}
mean(image.val1$label==rep(-1,nrow(image.val1)))
mean(image.val2$label==rep(-1,nrow(image.val2)))
mean(image.test1$label==rep(-1,nrow(image.test1)))
mean(image.test2$label==rep(-1,nrow(image.test2)))
```

Accuracy is around 60%. It is trivial that this classifier has a high accuracy when there is almost no cloud in the image.

### (c)

```{r, variable importance}
detect_difference <- function(x,v1,v2){
  abs(mean(v1<x)-mean(v2<x))
}

differences <- c()

for (i in seq(4,11,1)){
  min.max <- as.numeric(quantile(image.valid[,i],c(0,1))) # return min and max of a feature
  
  differences <- c(differences,1+max(sapply(seq(min.max[1],min.max[2],length.out=1000),
                                f2,
                                v1=image1.valid%>%filter(label==-1)%>%.[i]%>%unlist(),
                                v2=image1.valid%>%filter(label==1)%>%.[i]%>%unlist())))
  }


differences
```

Therefore, the first three features incorporate more information. 