## Question 3

### training/testing split

```{r, test-train splitting - kmeans}
set.seed(1223)
# split testing - 20%
# kmeans method
coordinates = image.valid %>% dplyr::select(x, y)

class.kmeans = folds.Kmean(coordinates, 5)

# the second one as testing - with the most balanced proportion
test.kmeans = image.valid[which(class.kmeans == 2), ]
# train
train.kmeans = image.valid[which(class.kmeans != 2), ]

# prepare features, labels and coordinates
train.kmeans.features = train.kmeans %>% dplyr::select(-x, -y, -label, -image)
train.kmeans.labels = train.kmeans %>% dplyr::pull(label)
train.kmeans.coordinates = train.kmeans %>% dplyr::select(x, y)

# testing
test.kmeans.features = test.kmeans %>% dplyr::select(-x, -y, -label, -image)
test.kmeans.labels = test.kmeans %>% dplyr::pull(label)
test.kmeans.coordinates = test.kmeans %>% dplyr::select(x, y)
```

```{r, test-train splitting - vsplit}
set.seed(1223)
# split testing - 20%
# vsplit method

class.vsplit = folds.Vsplit(coordinates, 5)

# the fourth one as testing - with the most balanced proportion
test.vsplit = image.valid[which(class.vsplit == 4), ]
# train
train.vsplit = image.valid[which(class.vsplit != 4), ]

# prepare features, labels and coordinates
train.vsplit.features = train.vsplit %>% dplyr::select(-x, -y, -label, -image)
train.vsplit.labels = train.vsplit %>% dplyr::pull(label)
train.vsplit.coordinates = train.vsplit %>% dplyr::select(x, y)

# test
test.vsplit.features = test.vsplit %>% dplyr::select(-x, -y, -label, -image)
test.vsplit.labels = test.vsplit %>% dplyr::pull(label)
test.vsplit.coordinates = test.vsplit %>% dplyr::select(x, y)
```

### Modelling

#### Random Forest - kmeans

```{r, random forest kmeans}
set.seed(1223)
# testing random forest
results.rf <- CVmaster(classifier = "randomForest", 
                       train.kmeans.features, 
                       train.kmeans.labels, 
                       K = 10, 
                       loss_method = loss.mean, 
                       split_method = folds.Kmean, 
                       train.kmeans.coordinates, 
                       tree.num = seq(300,350,50), 
                       tree.depth = seq(5,6,1))

# probabilistic prediction, need cutoff values to complete classification
fit.rf <- ranger::ranger(label~NDAI+SD+CORR+DF+CF+BF+AF+AN,
            data=train.kmeans,
            num.trees =results.rf$tree_num,
            max.depth = results.rf$tree_depth,
            num.threads = parallel::detectCores()-1,
            classification = TRUE,
            probability = TRUE,
            seed=1223)


# store predicted probabilities of labels equal to 1
prob.pred.rf <- predict(fit.rf,data=test.kmeans)$predictions[,1]

# ROC curve
# the threshold with highest sensitivity+specificity is printed in the plot
plot.roc(test.kmeans$label,prob.pred.rf,percent=TRUE,thresholds='best',print.thres="best",legacy.axes = T,auc.polygon = T) 
```

#### Random Forest - vsplit

```{r, random forest vsplit}
set.seed(1223)
# testing random forest
results.rfv <- CVmaster(classifier = "randomForest", 
                       train.vsplit.features, 
                       train.vsplit.labels, 
                       K = 10, 
                       loss_method = loss.mean, 
                       split_method = folds.Vsplit, 
                       train.vsplit.coordinates, 
                       tree.num = seq(300,350,50), 
                       tree.depth = seq(5,6,1))

# probabilistic prediction, need cutoff values to complete classification
fit.rfv <- ranger::ranger(label~NDAI+SD+CORR+DF+CF+BF+AF+AN,
            data=train.vsplit,
            num.trees =results.rfv$tree_num,
            max.depth = results.rfv$tree_depth,
            num.threads = parallel::detectCores()-1,
            classification = TRUE,
            probability = TRUE,
            seed=1223)


# store predicted probabilities of labels equal to 1
prob.pred.rfv <- predict(fit.rfv,data=test.vsplit)$predictions[,1]

# ROC curve
# the threshold with highest sensitivity+specificity is printed in the plot
plot.roc(test.vsplit$label,prob.pred.rfv,percent=TRUE,thresholds='best',print.thres="best",legacy.axes = T,auc.polygon = T) 
```

#### LDA - kmeans

```{r, LDA kmeans}
set.seed(1223)
results.lda = CVmaster("LDA", train.kmeans.features, train.kmeans.labels, K = 10, loss.mean, folds.Kmean, train.kmeans.coordinates)

LDA = lda(train.kmeans.labels ~ ., data = train.kmeans.features)
predict_LDA = predict(LDA, test.kmeans.features)$posterior[, 2]

# ROC curve
# the threshold with highest sensitivity+specificity is printed in the plot
plot.roc(test.kmeans$label,predict_LDA,percent=TRUE,thresholds='best',print.thres="best",legacy.axes = T,auc.polygon = T) 
results.lda
```

#### LDA - vsplit

```{r, LDA vsplit}
set.seed(1223)
results.ldav = CVmaster("LDA", train.vsplit.features, train.vsplit.labels, K = 10, loss.mean, folds.Kmean, train.vsplit.coordinates)

LDAv = lda(train.vsplit.labels ~ ., data = train.vsplit.features)
predict_LDAv = predict(LDAv, test.vsplit.features)$posterior[, 2]

# ROC curve
# the threshold with highest sensitivity+specificity is printed in the plot
plot.roc(test.vsplit$label,predict_LDAv,percent=TRUE,thresholds='best',print.thres="best",legacy.axes = T,auc.polygon = T) 
results.ldav
```

#### QDA - kmeans

```{r, QDA kmeans}
set.seed(1223)
results.qda = CVmaster("QDA", train.kmeans.features, train.kmeans.labels, K = 10, loss.mean, folds.Kmean, train.kmeans.coordinates)

QDA = qda(train.kmeans.labels ~ ., data = train.kmeans.features)
predict_QDA = predict(QDA, test.kmeans.features)$posterior[, 2]

# ROC curve
# the threshold with highest sensitivity+specificity is printed in the plot
plot.roc(test.kmeans$label,predict_QDA,percent=TRUE,thresholds='best',print.thres="best",legacy.axes = T,auc.polygon = T) 
results.qda
```

#### QDA - vsplit

```{r, QDA vsplit}
set.seed(1223)
results.qdav = CVmaster("QDA", train.vsplit.features, train.vsplit.labels, K = 10, loss.mean, folds.Kmean, train.vsplit.coordinates)

QDAv = qda(train.vsplit.labels ~ ., data = train.vsplit.features)
predict_QDAv = predict(QDAv, test.vsplit.features)$posterior[, 2]

# ROC curve
# the threshold with highest sensitivity+specificity is printed in the plot
plot.roc(test.vsplit$label,predict_QDAv,percent=TRUE,thresholds='best',print.thres="best",legacy.axes = T,auc.polygon = T) 
results.qdav
```


#### Logistics Regression - kmeans

```{r, Logistics Regression - kmeans}
set.seed(1223)
results.log = CVmaster("Logistic", train.kmeans.features, train.kmeans.labels, K = 10, loss.mean, folds.Kmean, train.kmeans.coordinates)

#encode labels to fit logistics regression
test.kmeans$label[test.kmeans$label == -1] = 0

LOG = glm(label ~ NDAI+SD+CORR+DF+CF+BF+AF+AN, family = "binomial",data = test.kmeans)
predict_log = predict(LOG, test.kmeans, type = "response")

# ROC curve
# the threshold with highest sensitivity+specificity is printed in the plot
# error
plot.roc(test.kmeans$label, predict_log, percent=TRUE,thresholds='best',print.thres="best",legacy.axes = T,auc.polygon = T)
```

#### Logistics Regression - vsplit

```{r, Logistics Regression - vsplit}
set.seed(1223)
results.logv = CVmaster("Logistic", train.vsplit.features, train.vsplit.labels, K = 10, loss.mean, folds.Vsplit, train.vsplit.coordinates)

#encode labels to fit logistics regression
test.vsplit$label[test.vsplit$label == -1] = 0

LOGv = glm(label ~ NDAI+SD+CORR+DF+CF+BF+AF+AN, family = "binomial",data = test.vsplit)
predict_logv = predict(LOGv, test.vsplit, type = "response")

# ROC curve
# the threshold with highest sensitivity+specificity is printed in the plot
# error
plot.roc(test.vsplit$label, predict_logv, percent=TRUE,thresholds='best',print.thres="best",legacy.axes = T,auc.polygon = T)
```

#### Naive Bayes - kmeans

```{r, Naive Bayes - kmeans,warning=FALSE}
set.seed(1223)
results.nb = CVmaster("naiveBayes", train.kmeans.features, train.kmeans.labels, K = 10, loss.mean, folds.Kmean, train.kmeans.coordinates)

# labels should be character
train.kmeans$label <- as.character(train.kmeans$label)
test.kmeans$label <- as.character(test.kmeans$label)

fit <- naivebayes::naive_bayes(label~NDAI+SD+CORR+DF+CF+BF+AF+AN,data = train.kmeans,usekernel = T)
predict_nb <- predict(fit,test.kmeans,type='prob')[,2]

# ROC curve
# the threshold with highest sensitivity+specificity is printed in the plot
# error
plot.roc(test.kmeans$label, predict_nb, percent=TRUE,thresholds='best',print.thres="best",legacy.axes = T,auc.polygon = T)
```

#### Naive Bayes - vsplit

```{r, Naive Bayes - vsplit}
set.seed(1223)
results.nbv = CVmaster("naiveBayes", train.vsplit.features, train.vsplit.labels, K = 10, loss.mean, folds.Vsplit, train.vsplit.coordinates)

# labels should be character
train.vsplit$label <- as.character(train.vsplit$label)
test.vsplit$label <- as.character(test.vsplit$label)

fit <- naivebayes::naive_bayes(label~NDAI+SD+CORR+DF+CF+BF+AF+AN,data = train.vsplit,usekernel = T)
predict_nbv <- predict(fit,test.vsplit,type='prob')[,2]

# ROC curve
# the threshold with highest sensitivity+specificity is printed in the plot
# error
plot.roc(test.vsplit$label, predict_nbv, percent=TRUE,thresholds='best',print.thres="best",legacy.axes = T,auc.polygon = T)
```

#### Extreme Gradient Boosting (XGBoost) - kmeans

```{r, XGBoost - kmeans}
set.seed(1223)
# testing random forest
results.xgb <- CVmaster(classifier = "xgboost", 
                       train.kmeans.features, 
                       train.kmeans.labels, 
                       K = 10, 
                       loss_method = loss.mean, 
                       split_method = folds.Kmean, 
                       train.kmeans.coordinates, 
                       tree.depth = seq(2,3,1),
                       eta = seq(0.2, 0.4, 0.1))

# encoding
train.kmeans.labels = replace(train.kmeans.labels, train.kmeans.labels == -1, 0)
dtrain = xgb.DMatrix(data = as.matrix(train.kmeans.features), label = train.kmeans.labels)

# probabilistic prediction, need cutoff values to complete classification
fit.xgb <- xgboost(data = dtrain, 
                   max.depth = results.xgb$tree_depth, 
                   eta = results.xgb$eta, 
                   nrounds = 2,
                   objective = "binary:logistic",
                   verbose = 0)
predict_XGB = predict(fit.xgb, as.matrix(test.kmeans.features))


# ROC curve
# the threshold with highest sensitivity+specificity is printed in the plot
plot.roc(test.kmeans$label,predict_XGB,percent=TRUE,thresholds='best',print.thres="best",legacy.axes = T,auc.polygon = T) 
```

#### Extreme Gradient Boosting (XGBoost) - vsplit

```{r, XGBoost - vsplit}
set.seed(1223)
# testing random forest
results.xgbv <- CVmaster(classifier = "xgboost", 
                       train.vsplit.features, 
                       train.vsplit.labels, 
                       K = 10, 
                       loss_method = loss.mean, 
                       split_method = folds.Vsplit, 
                       train.vsplit.coordinates, 
                       tree.depth = seq(2,3,1),
                       eta = seq(0.2, 0.4, 0.1))

# encoding
train.vsplit.labels = replace(train.vsplit.labels, train.vsplit.labels == -1, 0)
dtrain = xgb.DMatrix(data = as.matrix(train.vsplit.features), label = train.vsplit.labels)

# probabilistic prediction, need cutoff values to complete classification
fit.xgbv <- xgboost(data = dtrain, 
                   max.depth = results.xgb$tree_depth, 
                   eta = results.xgb$eta, 
                   nrounds = 2,
                   objective = "binary:logistic",
                   verbose = 0)
predict_XGBv = predict(fit.xgbv, as.matrix(test.vsplit.features))


# ROC curve
# the threshold with highest sensitivity+specificity is printed in the plot
plot.roc(test.vsplit$label,predict_XGBv,percent=TRUE,thresholds='best',print.thres="best",legacy.axes = T,auc.polygon = T) 
```

#### Combined ROC
```{r, Kmeans}
# ROC curves
roc.obj.rf <- roc(test.kmeans$label, prob.pred.rf)
roc.obj.logis <- roc(test.kmeans$label, predict_log)
roc.obj.lda = roc(test.kmeans$label, predict_LDA)
roc.obj.qda = roc(test.kmeans$label, predict_QDA)
roc.obj.nb = roc(test.kmeans$label, predict_nb)
roc.obj.xgb = roc(test.kmeans$label, predict_XGB)

coords(roc.obj.logis, "best", ret=c("threshold", "specificity", "sensitivity"))
coords(roc.obj.rf, "best", ret=c("threshold", "specificity", "sensitivity"))
coords(roc.obj.lda, "best", ret=c("threshold", "specificity", "sensitivity"))
coords(roc.obj.qda, "best", ret=c("threshold", "specificity", "sensitivity"))
coords(roc.obj.nb, "best", ret=c("threshold", "specificity", "sensitivity"))
coords(roc.obj.xgb, "best", ret=c("threshold", "specificity", "sensitivity"))

ggroc(list(logistic=roc.obj.logis,randomForest=roc.obj.rf, LDA=roc.obj.lda, QDA=roc.obj.qda, naiveBayes=roc.obj.nb, XGBoost = roc.obj.xgb)) +
  geom_point(aes(x=0.9325902, y=0.9628435), colour="lightcoral") +
  geom_point(aes(x=0.8592957, y=0.9427171), colour="tan") +
  geom_point(aes(x=0.8361304, y=0.8522771), colour="green") +
  geom_point(aes(x=0.8678581, y=0.9133445), colour="springgreen3") +
  geom_point(aes(x=0.8983716, y=0.9516191), colour="lightblue") +
  geom_point(aes(x=0.8518542, y=0.9465875), colour="violet") +
  labs(fill = "Dose (mg)") +
  theme_minimal()

# 0.359, 0.260, 0.278, 0.013, 0.047, 0.290
```


```{r, vsplit}
# ROC curves
roc.obj.rfv <- roc(test.vsplit$label, prob.pred.rfv)
roc.obj.logisv <- roc(test.vsplit$label, predict_logv)
roc.obj.ldav = roc(test.vsplit$label, predict_LDAv)
roc.obj.qdav = roc(test.vsplit$label, predict_QDAv)
roc.obj.nbv = roc(test.vsplit$label, predict_nbv)
roc.obj.xgbv = roc(test.vsplit$label, predict_XGBv)

coords(roc.obj.logisv, "best", ret=c("threshold", "specificity", "sensitivity"))
coords(roc.obj.rfv, "best", ret=c("threshold", "specificity", "sensitivity"))
coords(roc.obj.ldav, "best", ret=c("threshold", "specificity", "sensitivity"))
coords(roc.obj.qdav, "best", ret=c("threshold", "specificity", "sensitivity"))
coords(roc.obj.nbv, "best", ret=c("threshold", "specificity", "sensitivity"))
coords(roc.obj.xgbv, "best", ret=c("threshold", "specificity", "sensitivity"))

ggroc(list(logistic=roc.obj.logisv,randomForest=roc.obj.rfv, LDA=roc.obj.ldav, QDA=roc.obj.qdav, naiveBayes=roc.obj.nbv, XGBoost = roc.obj.xgbv)) +
  geom_point(aes(x=0.9184395, y=0.9829825), colour="tan") +
  geom_point(aes(x=0.9563934, y=0.972807), colour="lightcoral") +
  geom_point(aes(x=0.8992537, y=0.9645614), colour="green") +
  geom_point(aes(x=0.9193306, y=0.9938596), colour="violet") +
  geom_point(aes(x=0.8173869, y=0.9770175), colour="lightblue") +
  geom_point(aes(x=0.9218646, y=0.9875439), colour="springgreen3") +
  labs(fill = "Dose (mg)") +
  theme_minimal()

# 0.129, 0.580, 0.181, 0.013, 0.000329, 0.481
```



```{r}
results.rf
results.rfv
results.xgb
results.xgbv
```

```{r}
results.log
results.lda
results.qda
results.nb
results.rf$cv_loss
results.xgb$cv_loss

mean(results.log)
mean(results.lda)
mean(results.qda)
mean(results.nb)
mean(results.rf$cv_loss)
mean(results.xgb$cv_loss)

results.logv
results.ldav
results.qdav
results.nbv
results.rfv$cv_loss
results.xgbv$cv_loss

mean(results.logv)
mean(results.ldav)
mean(results.qdav)
mean(results.nbv)
mean(results.rfv$cv_loss)
mean(results.xgbv$cv_loss)
```

