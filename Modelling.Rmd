## Question 3

### training/testing split

```{r, test-train splitting - kmeans}
set.seed(1223)
# split testing - 20%
# kmeans method
coordinates = image.valid %>% dplyr::select(x, y)

class.kmeans = folds.Kmean(coordinates, 5)

# the second one as testing - with the most balanced proportion
test.kmeans = image.valid[which(class.kmeans == 2), ]
# train
train.kmeans = image.valid[which(class.kmeans != 2), ]

# prepare features, labels and coordinates
train.kmeans.features = train.kmeans %>% dplyr::select(-x, -y, -label, -image)
train.kmeans.labels = train.kmeans %>% dplyr::pull(label)
train.kmeans.coordinates = train.kmeans %>% dplyr::select(x, y)

# testing
test.kmeans.features = test.kmeans %>% dplyr::select(-x, -y, -label, -image)
test.kmeans.labels = test.kmeans %>% dplyr::pull(label)
test.kmeans.coordinates = test.kmeans %>% dplyr::select(x, y)
```

```{r, test-train splitting - vsplit}
set.seed(1223)
# split testing - 20%
# vsplit method

class.vsplit = folds.Vsplit(coordinates, 5)

# the fourth one as testing - with the most balanced proportion
test.vsplit = image.valid[which(class.vsplit == 4), ]
# train
train.vsplit = image.valid[which(class.vsplit != 4), ]

# prepare features, labels and coordinates
train.vsplit.features = train.vsplit %>% dplyr::select(-x, -y, -label, -image)
train.vsplit.labels = train.vsplit %>% dplyr::pull(label)
train.vsplit.coordinates = train.vsplit %>% dplyr::select(x, y)

# test
test.vsplit.features = test.vsplit %>% dplyr::select(-x, -y, -label, -image)
test.vsplit.labels = test.vsplit %>% dplyr::pull(label)
test.vsplit.coordinates = test.vsplit %>% dplyr::select(x, y)
```

### Modelling

#### Random Forest - kmeans

```{r, random forest kmeans}
set.seed(1223)
# testing random forest
results.rf <- CVmaster(classifier = "randomForest", 
                       train.kmeans.features, 
                       train.kmeans.labels, 
                       K = 10, 
                       loss_method = loss.mean, 
                       split_method = folds.Kmean, 
                       train.kmeans.coordinates, 
                       tree.num = seq(300,350,50), 
                       tree.depth = seq(5,6,1))

# probabilistic prediction, need cutoff values to complete classification
fit.rf <- ranger::ranger(label~NDAI+SD+CORR+DF+CF+BF+AF+AN,
            data=train.kmeans,
            num.trees =results.rf$tree_num,
            max.depth = results.rf$tree_depth,
            num.threads = parallel::detectCores()-1,
            classification = TRUE,
            probability = TRUE,
            seed=1223)


# store predicted probabilities of labels equal to 1
prob.pred.rf <- predict(fit.rf,data=test.kmeans)$predictions[,1]

# ROC curve
# the threshold with highest sensitivity+specificity is printed in the plot
plot.roc(test.kmeans$label,prob.pred.rf,percent=TRUE,thresholds='best',print.thres="best",legacy.axes = T,auc.polygon = T) 
```

#### Random Forest - vsplit

```{r, random forest vsplit}
set.seed(1223)
# testing random forest
results.rfv <- CVmaster(classifier = "randomForest", 
                       train.vsplit.features, 
                       train.vsplit.labels, 
                       K = 10, 
                       loss_method = loss.mean, 
                       split_method = folds.Vsplit, 
                       train.vsplit.coordinates, 
                       tree.num = seq(300,350,50), 
                       tree.depth = seq(5,6,1))

# probabilistic prediction, need cutoff values to complete classification
fit.rfv <- ranger::ranger(label~NDAI+SD+CORR+DF+CF+BF+AF+AN,
            data=train.vsplit,
            num.trees =results.rfv$tree_num,
            max.depth = results.rfv$tree_depth,
            num.threads = parallel::detectCores()-1,
            classification = TRUE,
            probability = TRUE,
            seed=1223)


# store predicted probabilities of labels equal to 1
prob.pred.rfv <- predict(fit.rfv,data=test.vsplit)$predictions[,1]

# ROC curve
# the threshold with highest sensitivity+specificity is printed in the plot
plot.roc(test.vsplit$label,prob.pred.rfv,percent=TRUE,thresholds='best',print.thres="best",legacy.axes = T,auc.polygon = T) 
```

#### LDA - kmeans

```{r, LDA kmeans}
set.seed(1223)
results.lda = CVmaster("LDA", train.kmeans.features, train.kmeans.labels, K = 10, loss.mean, folds.Kmean, train.kmeans.coordinates)

LDA = lda(train.kmeans.labels ~ ., data = train.kmeans.features)
predict_LDA = predict(LDA, test.kmeans.features)$posterior[, 2]

# ROC curve
# the threshold with highest sensitivity+specificity is printed in the plot
plot.roc(test.kmeans$label,predict_LDA,percent=TRUE,thresholds='best',print.thres="best",legacy.axes = T,auc.polygon = T) 
results.lda
```

#### LDA - vsplit

```{r, LDA vsplit}
set.seed(1223)
results.ldav = CVmaster("LDA", train.vsplit.features, train.vsplit.labels, K = 10, loss.mean, folds.Kmean, train.vsplit.coordinates)

LDAv = lda(train.vsplit.labels ~ ., data = train.vsplit.features)
predict_LDAv = predict(LDAv, test.vsplit.features)$posterior[, 2]

# ROC curve
# the threshold with highest sensitivity+specificity is printed in the plot
plot.roc(test.vsplit$label,predict_LDAv,percent=TRUE,thresholds='best',print.thres="best",legacy.axes = T,auc.polygon = T) 
results.ldav
```

#### QDA - kmeans

```{r, QDA kmeans}
set.seed(1223)
results.qda = CVmaster("QDA", train.kmeans.features, train.kmeans.labels, K = 10, loss.mean, folds.Kmean, train.kmeans.coordinates)

QDA = qda(train.kmeans.labels ~ ., data = train.kmeans.features)
predict_QDA = predict(QDA, test.kmeans.features)$posterior[, 2]

# ROC curve
# the threshold with highest sensitivity+specificity is printed in the plot
plot.roc(test.kmeans$label,predict_QDA,percent=TRUE,thresholds='best',print.thres="best",legacy.axes = T,auc.polygon = T) 
results.qda
```

#### QDA - vsplit

```{r, QDA vsplit}
set.seed(1223)
results.qdav = CVmaster("QDA", train.vsplit.features, train.vsplit.labels, K = 10, loss.mean, folds.Kmean, train.vsplit.coordinates)

QDAv = qda(train.vsplit.labels ~ ., data = train.vsplit.features)
predict_QDAv = predict(QDAv, test.vsplit.features)$posterior[, 2]

# ROC curve
# the threshold with highest sensitivity+specificity is printed in the plot
plot.roc(test.vsplit$label,predict_QDAv,percent=TRUE,thresholds='best',print.thres="best",legacy.axes = T,auc.polygon = T) 
results.qdav
```


#### Logistics Regression - kmeans

```{r, Logistics Regression - kmeans}
set.seed(1223)
results.log = CVmaster("Logistic", train.kmeans.features, train.kmeans.labels, K = 10, loss.mean, folds.Kmean, train.kmeans.coordinates)

#encode labels to fit logistics regression
test.kmeans$label[test.kmeans$label == -1] = 0

LOG = glm(label ~ NDAI+SD+CORR+DF+CF+BF+AF+AN, family = "binomial",data = test.kmeans)
predict_log = predict(LOG, test.kmeans, type = "response")

# ROC curve
# the threshold with highest sensitivity+specificity is printed in the plot
# error
plot.roc(test.kmeans$label, predict_log, percent=TRUE,thresholds='best',print.thres="best",legacy.axes = T,auc.polygon = T)
```

#### Logistics Regression - vsplit

```{r, Logistics Regression - vsplit}
set.seed(1223)
results.logv = CVmaster("Logistic", train.vsplit.features, train.vsplit.labels, K = 10, loss.mean, folds.Vsplit, train.vsplit.coordinates)

#encode labels to fit logistics regression
test.vsplit$label[test.vsplit$label == -1] = 0

LOGv = glm(label ~ NDAI+SD+CORR+DF+CF+BF+AF+AN, family = "binomial",data = test.vsplit)
predict_logv = predict(LOGv, test.vsplit, type = "response")

# ROC curve
# the threshold with highest sensitivity+specificity is printed in the plot
# error
plot.roc(test.vsplit$label, predict_logv, percent=TRUE,thresholds='best',print.thres="best",legacy.axes = T,auc.polygon = T)
```

#### Extreme Gradient Boosting (XGBoost) - kmeans

```{r, XGBoost - kmeans}
set.seed(1223)
# testing random forest
results.xgb <- CVmaster(classifier = "xgboost", 
                       train.kmeans.features, 
                       train.kmeans.labels, 
                       K = 10, 
                       loss_method = loss.mean, 
                       split_method = folds.Kmean, 
                       train.kmeans.coordinates, 
                       tree.depth = seq(2,3,1),
                       eta = seq(0.2, 0.4, 0.1))

# encoding
train.kmeans.labels = replace(train.kmeans.labels, train.kmeans.labels == -1, 0)
dtrain = xgb.DMatrix(data = as.matrix(train.kmeans.features), label = train.kmeans.labels)

# probabilistic prediction, need cutoff values to complete classification
fit.xgb <- xgboost(data = dtrain, 
                   max.depth = results.xgb$tree_depth, 
                   eta = results.xgb$eta, 
                   nrounds = 2,
                   objective = "binary:logistic",
                   verbose = 0)
predict_XGB = predict(fit.xgb, as.matrix(test.kmeans.features))


# ROC curve
# the threshold with highest sensitivity+specificity is printed in the plot
plot.roc(test.kmeans$label,predict_XGB,percent=TRUE,thresholds='best',print.thres="best",legacy.axes = T,auc.polygon = T) 
```

#### Extreme Gradient Boosting (XGBoost) - vsplit

```{r, XGBoost - vsplit}
set.seed(1223)
# testing random forest
results.xgbv <- CVmaster(classifier = "xgboost", 
                       train.vsplit.features, 
                       train.vsplit.labels, 
                       K = 10, 
                       loss_method = loss.mean, 
                       split_method = folds.Vsplit, 
                       train.vsplit.coordinates, 
                       tree.depth = seq(2,3,1),
                       eta = seq(0.2, 0.4, 0.1))

# encoding
train.vsplit.labels = replace(train.vsplit.labels, train.vsplit.labels == -1, 0)
dtrain = xgb.DMatrix(data = as.matrix(train.vsplit.features), label = train.vsplit.labels)

# probabilistic prediction, need cutoff values to complete classification
fit.xgbv <- xgboost(data = dtrain, 
                   max.depth = results.xgb$tree_depth, 
                   eta = results.xgb$eta, 
                   nrounds = 2,
                   objective = "binary:logistic",
                   verbose = 0)
predict_XGBv = predict(fit.xgbv, as.matrix(test.vsplit.features))


# ROC curve
# the threshold with highest sensitivity+specificity is printed in the plot
plot.roc(test.vsplit$label,predict_XGBv,percent=TRUE,thresholds='best',print.thres="best",legacy.axes = T,auc.polygon = T) 
```

#### Combined ROC (need to modify)

```{r}
# ROC curves
roc.obj.rf <- roc(test.vsplit$label, prob.pred.rf)
roc.obj.logis <- roc(test.vsplit$label, prob.pred.logis)

coords(roc.obj.logis, "best", ret=c("threshold", "specificity", "sensitivity"))
coords(roc.obj.rf, "best", ret=c("threshold", "specificity", "sensitivity"))
  

ggroc(list(logistic=roc.obj.logis,randomForest=roc.obj.rf)) +
  geom_point(aes(x=0.9188396, y=0.9847082), colour="red") +
  geom_point(aes(x=0.9107317, y=0.9719358), colour="blue") +
  annotate("text", x=0.9188396, y=1, label = "0.3564",size=3,color='red') +
  annotate("text", x=0.87, y=0.9719358, label = "0.3011",size=3,color='blue') +
  labs(fill = "Dose (mg)")


# another way to make roc curve
# plot(roc.obj.rf,
#      print.auc = TRUE,
#      auc.polygon = TRUE,
#      max.auc.polygon = TRUE,
#      legacy.axes = TRUE,
#      auc.polygon.col = "lightblue",
#       print.thres = "best",
#      print.auc.x = 0.4,
#      print.auc.y = 0.2
#      )
# 
# plot(roc.obj.logis,
#      print.auc = TRUE,
#      auc.polygon = TRUE,
#      max.auc.polygon = TRUE,
#      legacy.axes = TRUE,
#      auc.polygon.col = "lightyellow",
#       print.thres = "best",
#      print.auc.x = 0.4,
#      print.auc.y = 0.2,
#      add=TRUE
#      )

```





