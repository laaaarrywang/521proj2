## Question 3

### training/testing split

```{r, test-train splitting - kmeans}
set.seed(1223)
# split testing - 20%
# kmeans method
coordinates = image.valid %>% dplyr::select(x, y)

class.kmeans = folds.Kmean(coordinates, 5)

# the second one as testing - with the most balanced proportion
test.kmeans = image.valid[which(class.kmeans == 2), ]
# train
train.kmeans = image.valid[which(class.kmeans != 2), ]

# prepare features, labels and coordinates
train.kmeans.features = train.kmeans %>% dplyr::select(-x, -y, -label, -image)
train.kmeans.labels = train.kmeans %>% dplyr::pull(label)
train.kmeans.coordinates = train.kmeans %>% dplyr::select(x, y)

# testing
test.kmeans.features = test.kmeans %>% dplyr::select(-x, -y, -label, -image)
test.kmeans.labels = test.kmeans %>% dplyr::pull(label)
test.kmeans.coordinates = test.kmeans %>% dplyr::select(x, y)
```

```{r, test-train splitting - vsplit}
set.seed(1223)
# split testing - 20%
# vsplit method

class.vsplit = folds.Vsplit(coordinates, 5)

# the fourth one as testing - with the most balanced proportion
test.vsplit = image.valid[which(class.vsplit == 4), ]
# train
train.vsplit = image.valid[which(class.vsplit != 4), ]

# prepare features, labels and coordinates
train.vsplit.features = train.vsplit %>% dplyr::select(-x, -y, -label, -image)
train.vsplit.labels = train.vsplit %>% dplyr::pull(label)
train.vsplit.coordinates = train.vsplit %>% dplyr::select(x, y)

# test
test.vsplit.features = test.vsplit %>% dplyr::select(-x, -y, -label, -image)
test.vsplit.labels = test.vsplit %>% dplyr::pull(label)
test.vsplit.coordinates = test.vsplit %>% dplyr::select(x, y)
```

### Modelling

#### Random Forest - kmeans

```{r, random forest kmeans}
set.seed(1223)
# testing random forest
results.rf <- CVmaster(classifier = "randomForest", 
                       train.kmeans.features, 
                       train.kmeans.labels, 
                       K = 10, 
                       loss_method = loss.mean, 
                       split_method = folds.Kmean, 
                       train.kmeans.coordinates, 
                       tree.num = seq(200,600,50), 
                       tree.depth = seq(5,15,1))

# probabilistic prediction, need cutoff values to complete classification
fit.rf <- ranger::ranger(label~NDAI+SD+CORR+DF+CF+BF+AF+AN,
            data=train.kmeans,
            num.trees =results.rf$tree_num,
            max.depth = results.rf$tree_depth,
            num.threads = parallel::detectCores()-1,
            classification = TRUE,
            probability = TRUE,
            seed=1223)


# store predicted probabilities of labels equal to 1
prob.pred.rf <- predict(fit.rf,data=test.kmeans)$predictions[,1]

# ROC curve
# the threshold with highest sensitivity+specificity is printed in the plot
plot.roc(test.kmeans$label,prob.pred.rf,percent=TRUE,thresholds='best',print.thres="best",legacy.axes = T,auc.polygon = T) 
```

#### Random Forest - vsplit

```{r, random forest vsplit}
set.seed(1223)
# testing random forest
results.rfv <- CVmaster(classifier = "randomForest", 
                       train.vsplit.features, 
                       train.vsplit.labels, 
                       K = 10, 
                       loss_method = loss.mean, 
                       split_method = folds.Vsplit, 
                       train.vsplit.coordinates, 
                       tree.num = seq(200,600,50), 
                       tree.depth = seq(5,15,1))

# probabilistic prediction, need cutoff values to complete classification
fit.rfv <- ranger::ranger(label~NDAI+SD+CORR+DF+CF+BF+AF+AN,
            data=train.vsplit,
            num.trees =results.rfv$tree_num,
            max.depth = results.rfv$tree_depth,
            num.threads = parallel::detectCores()-1,
            classification = TRUE,
            probability = TRUE,
            seed=1223)


# store predicted probabilities of labels equal to 1
prob.pred.rfv <- predict(fit.rfv,data=test.vsplit)$predictions[,1]

# ROC curve
# the threshold with highest sensitivity+specificity is printed in the plot
plot.roc(test.vsplit$label,prob.pred.rfv,percent=TRUE,thresholds='best',print.thres="best",legacy.axes = T,auc.polygon = T) 
```

#### LDA - kmeans

```{r, LDA kmeans}
set.seed(1223)
results.lda = CVmaster("LDA", train.kmeans.features, train.kmeans.labels, K = 10, loss.mean, folds.Kmean, train.kmeans.coordinates)

LDA = lda(train.kmeans.labels ~ ., data = train.kmeans.features)
predict_LDA = predict(LDA, test.kmeans.features)$posterior[, 2]

# ROC curve
# the threshold with highest sensitivity+specificity is printed in the plot
plot.roc(test.kmeans$label,predict_LDA,percent=TRUE,thresholds='best',print.thres="best",legacy.axes = T,auc.polygon = T) 
results.lda
```

#### LDA - vsplit

```{r, LDA vsplit}
set.seed(1223)
results.ldav = CVmaster("LDA", train.vsplit.features, train.vsplit.labels, K = 10, loss.mean, folds.Kmean, train.vsplit.coordinates)

LDAv = lda(train.vsplit.labels ~ ., data = train.vsplit.features)
predict_LDAv = predict(LDAv, test.vsplit.features)$posterior[, 2]

# ROC curve
# the threshold with highest sensitivity+specificity is printed in the plot
plot.roc(test.vsplit$label,predict_LDAv,percent=TRUE,thresholds='best',print.thres="best",legacy.axes = T,auc.polygon = T) 
results.ldav
```

#### QDA - kmeans

```{r, QDA kmeans}
set.seed(1223)
results.qda = CVmaster("QDA", train.kmeans.features, train.kmeans.labels, K = 10, loss.mean, folds.Kmean, train.kmeans.coordinates)

QDA = qda(train.kmeans.labels ~ ., data = train.kmeans.features)
predict_QDA = predict(QDA, test.kmeans.features)$posterior[, 2]

# ROC curve
# the threshold with highest sensitivity+specificity is printed in the plot
plot.roc(test.kmeans$label,predict_QDA,percent=TRUE,thresholds='best',print.thres="best",legacy.axes = T,auc.polygon = T) 
results.qda
```

#### QDA - vsplit

```{r, QDA vsplit}
set.seed(1223)
results.qdav = CVmaster("QDA", train.vsplit.features, train.vsplit.labels, K = 10, loss.mean, folds.Kmean, train.vsplit.coordinates)

QDAv = qda(train.vsplit.labels ~ ., data = train.vsplit.features)
predict_QDAv = predict(QDAv, test.vsplit.features)$posterior[, 2]

# ROC curve
# the threshold with highest sensitivity+specificity is printed in the plot
plot.roc(test.vsplit$label,predict_QDAv,percent=TRUE,thresholds='best',print.thres="best",legacy.axes = T,auc.polygon = T) 
results.qdav
```


#### Logistics Regression - kmeans

```{r, Logistics Regression - kmeans}
set.seed(1223)
results.log = CVmaster("Logistic", train.kmeans.features, train.kmeans.labels, K = 10, loss.mean, folds.Kmean, train.kmeans.coordinates)

#encode labels to fit logistics regression
test.kmeans$label[test.kmeans$label == -1] = 0

LOG = glm(label ~ NDAI+SD+CORR+DF+CF+BF+AF+AN, family = "binomial",data = test.kmeans)
predict_log = predict(LOG, test.kmeans, type = "response")

# ROC curve
# the threshold with highest sensitivity+specificity is printed in the plot
# error
plot.roc(test.kmeans$label, predict_log, percent=TRUE,thresholds='best',print.thres="best",legacy.axes = T,auc.polygon = T)
```

#### Logistics Regression - vsplit

```{r, Logistics Regression - vsplit}
set.seed(1223)
results.logv = CVmaster("Logistic", train.vsplit.features, train.vsplit.labels, K = 10, loss.mean, folds.Vsplit, train.vsplit.coordinates)

#encode labels to fit logistics regression
test.vsplit$label[test.vsplit$label == -1] = 0

LOGv = glm(label ~ NDAI+SD+CORR+DF+CF+BF+AF+AN, family = "binomial",data = test.vsplit)
predict_logv = predict(LOGv, test.vsplit, type = "response")

# ROC curve
# the threshold with highest sensitivity+specificity is printed in the plot
# error
plot.roc(test.vsplit$label, predict_logv, percent=TRUE,thresholds='best',print.thres="best",legacy.axes = T,auc.polygon = T)
```

#### Naive Bayes - kmeans

```{r, Naive Bayes - kmeans,warning=FALSE}
set.seed(1223)
results.nb = CVmaster("naiveBayes", train.kmeans.features, train.kmeans.labels, K = 10, loss.mean, folds.Kmean, train.kmeans.coordinates)

# labels should be character
train.kmeans$label <- as.character(train.kmeans$label)
test.kmeans$label <- as.character(test.kmeans$label)

fit <- naivebayes::naive_bayes(label~NDAI+SD+CORR+DF+CF+BF+AF+AN,data = train.kmeans,usekernel = T)
predict_nb <- predict(fit,test.kmeans,type='prob')[,2]

# ROC curve
# the threshold with highest sensitivity+specificity is printed in the plot
# error
plot.roc(test.kmeans$label, predict_nb, percent=TRUE,thresholds='best',print.thres="best",legacy.axes = T,auc.polygon = T)
```

#### Naive Bayes - vsplit

```{r, Naive Bayes - vsplit}
set.seed(1223)
results.nbv = CVmaster("naiveBayes", train.vsplit.features, train.vsplit.labels, K = 10, loss.mean, folds.Vsplit, train.vsplit.coordinates)

# labels should be character
train.vsplit$label <- as.character(train.vsplit$label)
test.vsplit$label <- as.character(test.vsplit$label)

fit <- naivebayes::naive_bayes(label~NDAI+SD+CORR+DF+CF+BF+AF+AN,data = train.vsplit,usekernel = T)
predict_nbv <- predict(fit,test.vsplit,type='prob')[,2]

# ROC curve
# the threshold with highest sensitivity+specificity is printed in the plot
# error
plot.roc(test.vsplit$label, predict_nbv, percent=TRUE,thresholds='best',print.thres="best",legacy.axes = T,auc.polygon = T)
```

#### Extreme Gradient Boosting (XGBoost) - kmeans

```{r, XGBoost - kmeans}
set.seed(1223)
# testing random forest
results.xgb <- CVmaster(classifier = "xgboost", 
                       train.kmeans.features, 
                       train.kmeans.labels, 
                       K = 10, 
                       loss_method = loss.mean, 
                       split_method = folds.Kmean, 
                       train.kmeans.coordinates, 
                       tree.depth = seq(2,3,1),
                       eta = seq(0.2, 0.4, 0.1))

# encoding
train.kmeans.labels = replace(train.kmeans.labels, train.kmeans.labels == -1, 0)
dtrain = xgb.DMatrix(data = as.matrix(train.kmeans.features), label = train.kmeans.labels)

# probabilistic prediction, need cutoff values to complete classification
fit.xgb <- xgboost(data = dtrain, 
                   max.depth = results.xgb$tree_depth, 
                   eta = results.xgb$eta, 
                   nrounds = 2,
                   objective = "binary:logistic",
                   verbose = 0)
predict_XGB = predict(fit.xgb, as.matrix(test.kmeans.features))


# ROC curve
# the threshold with highest sensitivity+specificity is printed in the plot
plot.roc(test.kmeans$label,predict_XGB,percent=TRUE,thresholds='best',print.thres="best",legacy.axes = T,auc.polygon = T) 
```

#### Extreme Gradient Boosting (XGBoost) - vsplit

```{r, XGBoost - vsplit}
set.seed(1223)
# testing random forest
results.xgbv <- CVmaster(classifier = "xgboost", 
                       train.vsplit.features, 
                       train.vsplit.labels, 
                       K = 10, 
                       loss_method = loss.mean, 
                       split_method = folds.Vsplit, 
                       train.vsplit.coordinates, 
                       tree.depth = seq(2,3,1),
                       eta = seq(0.2, 0.4, 0.1))

# encoding
train.vsplit.labels = replace(train.vsplit.labels, train.vsplit.labels == -1, 0)
dtrain = xgb.DMatrix(data = as.matrix(train.vsplit.features), label = train.vsplit.labels)

# probabilistic prediction, need cutoff values to complete classification
fit.xgbv <- xgboost(data = dtrain, 
                   max.depth = results.xgb$tree_depth, 
                   eta = results.xgb$eta, 
                   nrounds = 2,
                   objective = "binary:logistic",
                   verbose = 0)
predict_XGBv = predict(fit.xgbv, as.matrix(test.vsplit.features))


# ROC curve
# the threshold with highest sensitivity+specificity is printed in the plot
plot.roc(test.vsplit$label,predict_XGBv,percent=TRUE,thresholds='best',print.thres="best",legacy.axes = T,auc.polygon = T) 
```

#### Combined ROC
```{r, Kmeans}
# ROC curves
roc.obj.rf <- roc(test.kmeans$label, prob.pred.rf)
roc.obj.logis <- roc(test.kmeans$label, predict_log)
roc.obj.lda = roc(test.kmeans$label, predict_LDA)
roc.obj.qda = roc(test.kmeans$label, predict_QDA)
roc.obj.nb = roc(test.kmeans$label, predict_nb)
roc.obj.xgb = roc(test.kmeans$label, predict_XGB)

clogis = coords(roc.obj.logis, "best", ret=c("threshold", "specificity", "sensitivity"))
crf = coords(roc.obj.rf, "best", ret=c("threshold", "specificity", "sensitivity"))
clda = coords(roc.obj.lda, "best", ret=c("threshold", "specificity", "sensitivity"))
cqda = coords(roc.obj.qda, "best", ret=c("threshold", "specificity", "sensitivity"))
cnb = coords(roc.obj.nb, "best", ret=c("threshold", "specificity", "sensitivity"))
cxgb = coords(roc.obj.xgb, "best", ret=c("threshold", "specificity", "sensitivity"))

ggroc(list(logistic=roc.obj.logis,randomForest=roc.obj.rf, LDA=roc.obj.lda, QDA=roc.obj.qda, naiveBayes=roc.obj.nb, XGBoost = roc.obj.xgb)) +
  geom_point(aes(x=clogis$specificity, y=clogis$sensitivity), colour="lightcoral") +
  geom_point(aes(x=crf$specificity, y=crf$sensitivity), colour="tan") +
  geom_point(aes(x=clda$specificity, y=clda$sensitivity), colour="green") +
  geom_point(aes(x=cqda$specificity, y=cqda$sensitivity), colour="springgreen3") +
  geom_point(aes(x=cnb$specificity, y=cnb$sensitivity), colour="lightblue") +
  geom_point(aes(x=cxgb$specificity, y=cxgb$sensitivity), colour="violet") +
  labs(fill = "Dose (mg)")
```


```{r, vsplit}
# ROC curves
roc.obj.rfv <- roc(test.vsplit$label, prob.pred.rfv)
roc.obj.logisv <- roc(test.vsplit$label, predict_logv)
roc.obj.ldav = roc(test.vsplit$label, predict_LDAv)
roc.obj.qdav = roc(test.vsplit$label, predict_QDAv)
roc.obj.nbv = roc(test.vsplit$label, predict_nbv)
roc.obj.xgbv = roc(test.vsplit$label, predict_XGBv)

clogisv = coords(roc.obj.logisv, "best", ret=c("threshold", "specificity", "sensitivity"))
crfv = coords(roc.obj.rfv, "best", ret=c("threshold", "specificity", "sensitivity"))
cldav = coords(roc.obj.ldav, "best", ret=c("threshold", "specificity", "sensitivity"))
cqdav = coords(roc.obj.qdav, "best", ret=c("threshold", "specificity", "sensitivity"))
cnbv = coords(roc.obj.nbv, "best", ret=c("threshold", "specificity", "sensitivity"))
cxgbv = coords(roc.obj.xgbv, "best", ret=c("threshold", "specificity", "sensitivity"))

ggroc(list(logistic=roc.obj.logisv,randomForest=roc.obj.rfv, LDA=roc.obj.ldav, QDA=roc.obj.qdav, naiveBayes=roc.obj.nbv, XGBoost = roc.obj.xgbv)) +
  geom_point(aes(x=clogisv$specificity, y=clogisv$sensitivity), colour="lightcoral") +
  geom_point(aes(x=crfv$specificity, y=crfv$sensitivity), colour="tan") +
  geom_point(aes(x=cldav$specificity, y=cldav$sensitivity), colour="green") +
  geom_point(aes(x=cqdav$specificity, y=cqdav$sensitivity), colour="violet") +
  geom_point(aes(x=cnbv$specificity, y=cnbv$sensitivity), colour="lightblue") +
  geom_point(aes(x=cxgbv$specificity, y=cxgbv$sensitivity), colour="springgreen3") +
  labs(fill = "Dose (mg)") 
```

```{r, Find PPV and NPV}
# PPV = TP/Predicted Positive
pred_rf = ifelse(prob.pred.rf > crf$threshold	, 1, -1)
pred_log = ifelse(predict_log > clogis$threshold, 1, -1)
pred_lda = ifelse(predict_LDA > clda$threshold, 1, -1)
pred_qda = ifelse(predict_QDA > cqda$threshold, 1, -1)
pred_nb = ifelse(predict_nb > cnb$threshold, 1, -1)
pred_xgb = ifelse(predict_XGB > cxgb$threshold, 1, -1)

pred_rfv = ifelse(prob.pred.rfv > crfv$threshold, 0, 1)
pred_logv = ifelse(predict_logv > clogisv$threshold, 1, 0)
pred_ldav = ifelse(predict_LDAv > cldav$threshold, 1, 0)
pred_qdav = ifelse(predict_QDAv > cqdav$threshold, 1, 0)
pred_nbv = ifelse(predict_nbv > cnbv$threshold, 1, 0)
pred_xgbv = ifelse(predict_XGBv > cxgbv$threshold, 1, 0)

PPVrf = nrow(as.data.frame(cbind(pred_rf, test.kmeans$label)) %>% 
  dplyr::filter(pred_rf == 1 & V2 == 1))/nrow(as.data.frame(cbind(pred_rf, test.kmeans$label)) %>% 
  dplyr::filter(pred_rf == 1))
PPVlog = nrow(as.data.frame(cbind(pred_log, test.kmeans$label)) %>% 
  dplyr::filter(pred_log == 1 & V2 == 1))/nrow(as.data.frame(cbind(pred_log, test.kmeans$label)) %>% 
  dplyr::filter(pred_log == 1))
PPVlda = nrow(as.data.frame(cbind(pred_lda, test.kmeans$label)) %>% 
  dplyr::filter(pred_lda == 1 & V2 == 1))/nrow(as.data.frame(cbind(pred_lda, test.kmeans$label)) %>% 
  dplyr::filter(pred_lda == 1))
PPVqda = nrow(as.data.frame(cbind(pred_qda, test.kmeans$label)) %>% 
  dplyr::filter(pred_qda == 1 & V2 == 1))/nrow(as.data.frame(cbind(pred_qda, test.kmeans$label)) %>% 
  dplyr::filter(pred_qda == 1))
PPVnb = nrow(as.data.frame(cbind(pred_nb, test.kmeans$label)) %>% 
  dplyr::filter(pred_nb == 1 & V2 == 1))/nrow(as.data.frame(cbind(pred_nb, test.kmeans$label)) %>% 
  dplyr::filter(pred_nb == 1))
PPVxgb = nrow(as.data.frame(cbind(pred_xgb, test.kmeans$label)) %>% 
  dplyr::filter(pred_xgb == 1 & V2 == 1))/nrow(as.data.frame(cbind(pred_xgb, test.kmeans$label)) %>% 
  dplyr::filter(pred_xgb == 1))

PPVrfv = nrow(as.data.frame(cbind(pred_rfv, test.vsplit$label)) %>% 
  dplyr::filter(pred_rfv == 1 & V2 == 1))/nrow(as.data.frame(cbind(pred_rfv, test.vsplit$label)) %>% 
  dplyr::filter(pred_rfv == 1))
PPVlogv = nrow(as.data.frame(cbind(pred_logv, test.vsplit$label)) %>% 
  dplyr::filter(pred_logv == 1 & V2 == 1))/nrow(as.data.frame(cbind(pred_logv, test.vsplit$label)) %>% 
  dplyr::filter(pred_logv == 1))
PPVldav = nrow(as.data.frame(cbind(pred_ldav, test.vsplit$label)) %>% 
  dplyr::filter(pred_ldav == 1 & V2 == 1))/nrow(as.data.frame(cbind(pred_ldav, test.vsplit$label)) %>% 
  dplyr::filter(pred_ldav == 1))
PPVqdav = nrow(as.data.frame(cbind(pred_qdav, test.vsplit$label)) %>% 
  dplyr::filter(pred_qdav == 1 & V2 == 1))/nrow(as.data.frame(cbind(pred_qdav, test.vsplit$label)) %>% 
  dplyr::filter(pred_qdav == 1))
PPVnbv = nrow(as.data.frame(cbind(pred_nbv, test.vsplit$label)) %>% 
  dplyr::filter(pred_nbv == 1 & V2 == 1))/nrow(as.data.frame(cbind(pred_nbv, test.vsplit$label)) %>% 
  dplyr::filter(pred_nbv == 1))
PPVxgbv = nrow(as.data.frame(cbind(pred_xgbv, test.vsplit$label)) %>% 
  dplyr::filter(pred_xgbv == 1 & V2 == 1))/nrow(as.data.frame(cbind(pred_xgbv, test.vsplit$label)) %>% 
  dplyr::filter(pred_xgbv == 1))
```

```{r, NPV calculation}
NPVrf = nrow(as.data.frame(cbind(pred_rf, test.kmeans$label)) %>% 
  dplyr::filter(pred_rf == -1 & V2 == -1))/nrow(as.data.frame(cbind(pred_rf, test.kmeans$label)) %>% 
  dplyr::filter(pred_rf == -1))
NPVlog = nrow(as.data.frame(cbind(pred_log, test.kmeans$label)) %>% 
  dplyr::filter(pred_log == -1 & V2 == -1))/nrow(as.data.frame(cbind(pred_log, test.kmeans$label)) %>% 
  dplyr::filter(pred_log == -1))
NPVlda = nrow(as.data.frame(cbind(pred_lda, test.kmeans$label)) %>% 
  dplyr::filter(pred_lda == -1 & V2 == -1))/nrow(as.data.frame(cbind(pred_lda, test.kmeans$label)) %>% 
  dplyr::filter(pred_lda == -1))
NPVqda = nrow(as.data.frame(cbind(pred_qda, test.kmeans$label)) %>% 
  dplyr::filter(pred_qda == -1 & V2 == -1))/nrow(as.data.frame(cbind(pred_qda, test.kmeans$label)) %>% 
  dplyr::filter(pred_qda == -1))
NPVnb = nrow(as.data.frame(cbind(pred_nb, test.kmeans$label)) %>% 
  dplyr::filter(pred_nb ==-1 &  V2 == -1))/nrow(as.data.frame(cbind(pred_nb, test.kmeans$label)) %>% 
  dplyr::filter(pred_nb == -1))
NPVxgb = nrow(as.data.frame(cbind(pred_xgb, test.kmeans$label)) %>% 
  dplyr::filter(pred_xgb == -1 & V2 == -1))/nrow(as.data.frame(cbind(pred_xgb, test.kmeans$label)) %>% 
  dplyr::filter(pred_xgb == -1))

NPVrfv = nrow(as.data.frame(cbind(pred_rfv, test.vsplit$label)) %>% 
  dplyr::filter(pred_rfv == 0 & V2 == -1))/nrow(as.data.frame(cbind(pred_rfv, test.vsplit$label)) %>% 
  dplyr::filter(pred_rfv == 0))
NPVlogv = nrow(as.data.frame(cbind(pred_logv, test.vsplit$label)) %>% 
  dplyr::filter(pred_logv == 0 & V2 == -1))/nrow(as.data.frame(cbind(pred_logv, test.vsplit$label)) %>% 
  dplyr::filter(pred_logv == 0))
NPVldav = nrow(as.data.frame(cbind(pred_ldav, test.vsplit$label)) %>% 
  dplyr::filter(pred_ldav == 0 & V2 == -1))/nrow(as.data.frame(cbind(pred_ldav, test.vsplit$label)) %>% 
  dplyr::filter(pred_ldav == 0))
NPVqdav = nrow(as.data.frame(cbind(pred_qdav, test.vsplit$label)) %>% 
  dplyr::filter(pred_qdav == 0 & V2 == -1))/nrow(as.data.frame(cbind(pred_qdav, test.vsplit$label)) %>% 
  dplyr::filter(pred_qdav == 0))
NPVnbv = nrow(as.data.frame(cbind(pred_nbv, test.vsplit$label)) %>% 
  dplyr::filter(pred_nbv == 0 & V2 == -1))/nrow(as.data.frame(cbind(pred_nbv, test.vsplit$label)) %>% 
  dplyr::filter(pred_nbv == 0))
NPVxgbv = nrow(as.data.frame(cbind(pred_xgbv, test.vsplit$label)) %>% 
  dplyr::filter(pred_xgbv == 0 & V2 == -1))/nrow(as.data.frame(cbind(pred_xgbv, test.vsplit$label)) %>% 
  dplyr::filter(pred_xgbv == 0))
```

```{r, check PPV and NPV values}
# PPV - kmeans
PPVrf
PPVlog
PPVlda
PPVqda
PPVnb
PPVxgb
print(" ")
# PPV - vsplit
PPVrfv
PPVlogv
PPVldav
PPVqdav
PPVnbv
PPVxgbv
print(" ")
# NPV - kmeans
NPVrf
NPVlog
NPVlda
NPVqda
NPVnb
NPVxgb
print(" ")
# NPV - vsplit
NPVrfv
NPVlogv
NPVldav
NPVqdav
NPVnbv
NPVxgbv
```

```{r, Calculate F1 score}
# Kmeans
2*(PPVrf * crf$sensitivity)/(PPVrf + crf$sensitivity)
2*(PPVlog * clogis$sensitivity)/(PPVlog + clogis$sensitivity)
2*(PPVlda * clda$sensitivity)/(PPVlda + clda$sensitivity)
2*(PPVqda * cqda$sensitivity)/(PPVqda + cqda$sensitivity)
2*(PPVnb * cnb$sensitivity)/(PPVnb + cnb$sensitivity)
2*(PPVxgb * cxgb$sensitivity)/(PPVxgb + cxgb$sensitivity)
print(" ")
# Vsplit
2*(PPVrfv * crfv$sensitivity)/(PPVrfv + crfv$sensitivity)
2*(PPVlogv * clogisv$sensitivity)/(PPVlogv + clogisv$sensitivity)
2*(PPVldav * cldav$sensitivity)/(PPVldav + cldav$sensitivity)
2*(PPVqdav * cqdav$sensitivity)/(PPVqdav + cqdav$sensitivity)
2*(PPVnbv * cnbv$sensitivity)/(PPVnbv + cnbv$sensitivity)
2*(PPVxgbv * cxgbv$sensitivity)/(PPVxgbv + cxgbv$sensitivity)
```

```{r, test accuracy}
# Kmeans
mean(test.kmeans$label == ifelse(prob.pred.rf > crf$threshold	, 1, -1))
mean(test.kmeans$label == ifelse(predict_log > clogis$threshold, 1, -1))
mean(test.kmeans$label == ifelse(predict_LDA > clda$threshold, 1, -1))
mean(test.kmeans$label == ifelse(predict_QDA > cqda$threshold, 1, -1))
mean(test.kmeans$label == ifelse(predict_nb > cnb$threshold, 1, -1))
mean(test.kmeans$label == ifelse(predict_XGB > cxgb$threshold, 1, -1))
print(" ")
# Vsplit
mean(test.vsplit$label == ifelse(prob.pred.rfv > crfv$threshold, -1, 1))
mean(test.vsplit$label == ifelse(predict_logv > clogisv$threshold, 1, -1))
mean(test.vsplit$label == ifelse(predict_LDAv > cldav$threshold, 1, -1))
mean(test.vsplit$label == ifelse(predict_QDAv >cqdav$threshold, 1, -1))
mean(test.vsplit$label == ifelse(predict_nbv > cnbv$threshold, 1, -1))
mean(test.vsplit$label == ifelse(predict_XGBv > cxgbv$threshold, 1, -1))
```

```{r, check best parameters}
results.rf
results.rfv
results.xgb
results.xgbv
```

```{r, check CV error}
results.log
results.lda
results.qda
results.nb
results.rf$cv_loss
results.xgb$cv_loss

mean(results.log)
mean(results.lda)
mean(results.qda)
mean(results.nb)
mean(results.rf$cv_loss)
mean(results.xgb$cv_loss)

results.logv
results.ldav
results.qdav
results.nbv
results.rfv$cv_loss
results.xgbv$cv_loss

mean(results.logv)
mean(results.ldav)
mean(results.qdav)
mean(results.nbv)
mean(results.rfv$cv_loss)
mean(results.xgbv$cv_loss)
```

