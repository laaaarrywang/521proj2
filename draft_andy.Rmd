---
title: "Andy's Draft"
author: "Andy"
date: "12/03/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,cache=T)
library(tidyverse)
library(ggplot2)
library(GGally)
library(patchwork)
library(stats)
library(tidymodels)
library(rsample)
library(MASS)
library(xgboost)
```

## Data loading
```{r, results='hide'}
image1 = read.table("image_data/imagem1.txt", header=F)
colnames(image1) = c("y","x","label","NDAI","SD","CORR","DF","CF","BF","AF","AN")
image2 = read.table("image_data/imagem2.txt", header=F)
colnames(image2) = c("y","x","label","NDAI","SD","CORR","DF","CF","BF","AF","AN")
image3 = read.table("image_data/imagem3.txt", header=F)
colnames(image3) = c("y","x","label","NDAI","SD","CORR","DF","CF","BF","AF","AN")
```

```{r}
#From now on, we drop the samples that are not labelled.
image.valid = rbind(image1%>%filter(label!=0)%>%mutate(image=1),
                    image2%>%filter(label!=0)%>%mutate(image=2),
                    image3%>%filter(label!=0)%>%mutate(image=3)) %>%
  arrange(x)
```

### (d)

```{r}
# Write a generic cross validation (CV) function CVmaster in R that takes a generic classifier, training features, training labels, number of folds K and a loss function (at least classification accuracy should be there) as inputs and outputs the K-fold CV loss on the training set. Please remember to put it in your github folder in Section 5.

# split method 1: Kmeans
folds.Kmean = function(coordinate, K){
  return(kmeans(coordinate, K)$cluster)
}

# split method 2: 
folds.Vsplit = function(coordinate, K){
  # implicitly assumes that the x axis is sorted
  quantile = round(as.numeric(quantile(seq_len(nrow(coordinate)), seq(0, 1, 1/K))))
  class = c()
  for (k in 1:K)
    if (k != K)
      class = c(class, rep(k, quantile[k + 1] - quantile[k]))
  else
    class = c(class, rep(k, quantile[k + 1] - quantile[k] + 1))
  return (class)
}
```


```{r}
set.seed(1223)
# split testing - 20%
coordinate = image.valid %>% dplyr::select(x, y)
class.kmeans = folds.Kmean(coordinate, 5)
class.vsplit = folds.Vsplit(coordinate, 5)
# the fifth one as testing
test.kmeans = image.valid[which(class.kmeans == 5), ]
test.vsplit = image.valid[which(class.vsplit == 5), ]
# train
train.kmeans = image.valid[which(class.kmeans != 5), ]
train.vsplit = image.valid[which(class.vsplit != 5), ]
```

```{r}
# split training set according to features, labels and parameters
train.kmeans.features = train.kmeans %>% dplyr::select(-x, -y, -label, -image)
train.kmeans.labels = train.kmeans %>% dplyr::pull(label)
train.kmeans.coordinates = train.kmeans %>% dplyr::select(x, y)

train.vsplit.features = train.vsplit %>% dplyr::select(-x, -y, -label, -image)
train.vsplit.labels = train.vsplit %>% dplyr::pull(label)
train.vsplit.coordinates = train.vsplit %>% dplyr::select(x, y)
```

```{r}
loss.mean = function(label, predicted)
  return (mean(label != predicted))
```

```{r}
#' takes a generic classifier, training features, training labels, number of folds K and a loss function and output the K-fold CV loss on the training set
#' 
#' @param classifier 
#' @param features A data frame containing of all the features (with column names). 
#' @param labels A vector of labels. One-to-one correspondence with the features
#' @param K An integer. The number of folds.
#' @param loss_method A function that takes two inputs of data frames/vectors (classified labels and predicted labels)
#' @param split_method method used to split the data set (Kmeans or vertical split)
#' @param coordinate The coordinate of the input data, corresponds to features and labels
#' @returns A number: the K-fold CV loss on the training set
#' @examples
#' CVmaster(linear_reg, image.features, image.labels, 5, roc_auc)
CVmaster = function(classifier, features, labels, K = 5, loss_method, split_method, coordinate, ...){
  
  # get K folds
  fold = split_method(coordinate, K)
  
  # record loss
  loss = c()
  
  # for each k
  for(k in 1:K){
    # separate into training and validation
    position = which(fold == k)
    
    # features
    features.train = features[position, ]
    features.validate = features[position, ]
    
    # labels
    labels.train = labels[position]
    labels.validate = labels[position]
    
    # training
    if(classifier == "LDA"){
      LDA = lda(labels.train ~ ., data = features.train)
      predict_LDA = predict(LDA, features.validate)$class
      loss = c(loss, loss_method(predict_LDA, labels.validate))
    }
    else if(classifier == "QDA"){
      QDA = qda(labels.train ~ ., data = features.train)
      predict_QDA = predict(QDA, features.validate)$class
      loss = c(loss, loss_method(predict_QDA, labels.validate))
    }
    else if(classifier == "xgboost"){
      # currently not working
      labels.train = replace(labels.train, labels.train == -1, 0)
      labels.validate = replace(labels.validate, labels.validate == -1, 0)
      dtrain = xgb.DMatrix(data = as.matrix(features.train), label = labels.train)
      dvalidate = xgb.DMatrix(data = as.matrix(features.validate), label = labels.validate)
      XGB = xgboost(data = dtrain, ...)
      predict_XGB = predict(XGB, features.validate)
      loss = c(loss, loss_method(predict_XGB, labels.validate))
    }
    
  }
  
  # return the K-fold CV loss on the training set
  return (loss)
}
```

```{r}
set.seed(1223)
CVmaster("LDA", train.kmeans.features, train.kmeans.labels, K = 5, loss.mean, folds.Kmean, train.kmeans.coordinates)
CVmaster("QDA", train.kmeans.features, train.kmeans.labels, K = 5, loss.mean, folds.Kmean, train.kmeans.coordinates)
```

```{r}
CVmaster("xgboost", train.kmeans.features, train.kmeans.labels, K = 5, loss.mean, folds.Kmean, train.kmeans.coordinates, max.depth = 2, eta = 1, nthread = 2, nrounds = 2, objective = "binary:logistic", verbose = 0)
```

```{r}
train.kmeans.labels.XGB = replace(train.kmeans.labels, train.kmeans.labels == -1, 0)
dtrain = xgb.DMatrix(data = as.matrix(train.kmeans.features), label = train.kmeans.labels.XGB)
XGB = xgboost(data = dtrain, max.depth = 2, eta = 1, nthread = 2, nrounds = 2, objective = "binary:logistic", verbose = 0)

test.kmeans.features = test.kmeans %>% dplyr::select(-x, -y, -image, -label)
test.kmeans.labels = test.kmeans %>% dplyr::pull(label)
test.kmeans.labels.XGB = replace(test.kmeans.labels, test.kmeans.labels == -1, 0)
predicted_XGB = predict(XGB, as.matrix(test.kmeans.features))
```

```{r}
predicted_XGB = as.numeric(predicted_XGB > 0.5)
mean(predicted_XGB != test.kmeans.labels.XGB)
```

<!-- Rubbish attempts for tidymodel - decide not to use -->
<!-- ```{r} -->
<!-- temp = image1.valid %>% -->
<!--   filter(x < quantile(image1.valid$x, 0.01)) -->
<!-- temp2 = image1.valid %>% -->
<!--   filter(x < quantile(image1.valid$x, 0.02)) %>% -->
<!--   filter(x > quantile(image1.valid$x, 0.01)) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- ind = list(analysis = seq(nrow(temp)), assessment = nrow(temp) + seq(nrow(temp2))) -->
<!-- b = make_splits(ind, rbind(temp, temp2)) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- new_rset(b, id = as_tibble(data.frame(id = c("Fold1", "Fold2")))) -->
<!-- ``` -->










<!-- # another way to make roc curve -->
<!-- # plot(roc.obj.rf, -->
<!-- #      print.auc = TRUE, -->
<!-- #      auc.polygon = TRUE, -->
<!-- #      max.auc.polygon = TRUE, -->
<!-- #      legacy.axes = TRUE, -->
<!-- #      auc.polygon.col = "lightblue", -->
<!-- #       print.thres = "best", -->
<!-- #      print.auc.x = 0.4, -->
<!-- #      print.auc.y = 0.2 -->
<!-- #      ) -->
<!-- #  -->
<!-- # plot(roc.obj.logis, -->
<!-- #      print.auc = TRUE, -->
<!-- #      auc.polygon = TRUE, -->
<!-- #      max.auc.polygon = TRUE, -->
<!-- #      legacy.axes = TRUE, -->
<!-- #      auc.polygon.col = "lightyellow", -->
<!-- #       print.thres = "best", -->
<!-- #      print.auc.x = 0.4, -->
<!-- #      print.auc.y = 0.2, -->
<!-- #      add=TRUE -->
<!-- #      ) -->








