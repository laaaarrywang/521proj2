---
title: "STA521 PROJECT 2"
author: "Linxuan Wang, Jingan Zhou"
date: "11/28/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,cache=T)
library(tidyverse)
library(ggplot2)
library(GGally)
library(patchwork)
library(stats)
library(tidymodels)
```

## Data loading
```{r, results='hide'}
image1 = read.table("image_data/imagem1.txt", header=F)
colnames(image1) = c("y","x","label","NDAI","SD","CORR","DF","CF","BF","AF","AN")
image2 = read.table("image_data/imagem2.txt", header=F)
colnames(image2) = c("y","x","label","NDAI","SD","CORR","DF","CF","BF","AF","AN")
image3 = read.table("image_data/imagem3.txt", header=F)
colnames(image3) = c("y","x","label","NDAI","SD","CORR","DF","CF","BF","AF","AN")
```

## Question 1
### (a) half-page summary

### (b) summary and plot
```{r, summary of the data}

image1 %>%
  group_by(label) %>%
  summarise(count = n()) %>%
  mutate(percentage = count/sum(count))

image2 %>%
  group_by(label) %>%
  summarise(count = n()) %>%
  mutate(percentage = count/sum(count))

image3 %>%
  group_by(label) %>%
  summarise(count = n()) %>%
  mutate(percentage = count/sum(count))
```
label: -1 (no cloud); 0; 1 (cloud)
image1: 37.3, 28.6, 34.1
image2: 43.8. 38.5, 17.8
image3: 29.3, 52.3, 18.4


```{r, plot}
image1 %>%
  ggplot(aes(x,y)) +
  geom_raster(aes(fill=label)) +
  scale_fill_gradientn(colours = c('grey','black','white'))

image2 %>%
  ggplot(aes(x,y)) +
  geom_raster(aes(fill=label)) +
  scale_fill_gradientn(colours = c('grey','black','white'))

image3 %>%
  ggplot(aes(x,y)) +
  geom_raster(aes(fill=label)) +
  scale_fill_gradientn(colours = c('grey','black','white'))
```
We observe several trends and patterns: (i) Areas of both clouds and surfaces tend to be large. In other words, there is almost no slim slices of clouds and surfaces. This is reflected in the dataset as pixels near to each other tend to have the same labels, unless they are at the boundaries between classes with different labels; (ii) Almost no "black holes" exist in either clouds or surfaces. This pattern shows somewhat continuity of cloud and surface distribution: 
 
The i.i.d. assumption is not justified for the samples. Since we have observed large areas of clouds and surfaces, given a pixel in the image, in most of the scenarios it has the same label as its neighbors, which implies that pixels near each other are highly dependent and tend to have the same label.

### (c) Visual and quantitative EDA

Remark: From Figure 3 in the paper, we could see that the data collected for three different images are consisted of roughly similar components. From the visualization in (b), we could see that the cluster pattern exists in all three images. Since the attributes for these images are similar, it makes sense to combine the attributes when performing EDA analysis.

#### (i)
```{r,cache=TRUE, pairwise relationship}
image_binded <- rbind(image1,image2,image3)
ggpairs(image_binded,columns = c("NDAI", "SD", "CORR","DF","CF","BF","AF","AN"))
```

We could see some correlation between SD and NDAI (cor = 0.631). The correlation between SD and CORR and that between CORR and NADI are not as significant.

The correlations between NDAI, SD, CORR and the radiance features increase as the angles decrease (from DF to AN) though all the correlations are low (almost all with <0.5 correlations).

For the radiance, we could see that the correlations between those with smaller angles are higher than those with larger angles given similar angle difference. The correlation decreases as the difference between angles increases but all correlations are at least 0.548.

#### (ii)
```{r}
p1 = image_binded %>% mutate(label = as.character(label)) %>%
  ggplot() +
  geom_density(aes(x=SD, fill=label),alpha=0.6)

p2 = image_binded %>% mutate(label = as.character(label),
                        log_SD = log(SD)) %>%
  ggplot() +
  geom_density(aes(x=log_SD, fill=label),alpha=0.6)

p3 = image_binded %>% mutate(label = as.character(label)) %>%
  ggplot() +
  geom_density(aes(x=NDAI, fill=label),alpha=0.6)

p4 = image_binded %>% mutate(label = as.character(label)) %>%
  ggplot() +
  geom_density(aes(x=CORR, fill=label),alpha=0.6)

p5 = image_binded %>% mutate(label = as.character(label)) %>%
  ggplot() +
  geom_density(aes(x=DF, fill=label),alpha=0.6)

p6 = image_binded %>% mutate(label = as.character(label)) %>%
  ggplot() +
  geom_density(aes(x=CF, fill=label),alpha=0.6)

p7 = image_binded %>% mutate(label = as.character(label)) %>%
  ggplot() +
  geom_density(aes(x=BF, fill=label),alpha=0.6)

p8 = image_binded %>% mutate(label = as.character(label)) %>%
  ggplot() +
  geom_density(aes(x=AF, fill=label),alpha=0.6)

p9 = image_binded %>% mutate(label = as.character(label)) %>%
  ggplot() +
  geom_density(aes(x=AN, fill=label),alpha=0.6)

p1+p2+p3+p4

p5+p6+p7+p8+p9
```
For the three essential engineered features (SD,NDAI,CORR): samples labelled "no cloud" typically have lower SD, NDAI and CORR.  

For the five radiance features: samples labelled "no cloud" typically have two peaks, and are more tightly distributed.
samples labelled "cloud" are more sparsely distributed and negatively skewed. The peak for samples labelled "cloud" normally lies between the peaks for samples labelled "no cloud"

*** Side remark - Also, it appears that almost all data points with low NDAI are labelled as "no cloud", while a few are undetermined and almost none are labelled as "cloud".

## Question 2

### (a)
Ideally, we want our training and validation sets to be as different as possible since the real pictures can be really heterogeneous. If we just use random number generator to specify the samples that should be thrown to the training and validation set respectively, the training "image" and validation "image" will intersect with each other severely. High degree of intersection leads to similarity between training and validation images because samples that are near to each other tend to have same labels. In that case, the validation error is not a good proxy for testing error, and thus testing error will possibly be much higher than validation error when we apply the classification model on a new image.

To take the fact that data is not i.i.d. into account, we would like to split the image into several "blocks" that are treated as sub-images, such that each block only borders other blocks at the boundary. This data splitting rule significantly reduces the similarity between training set and validation set as well as the testing set. In the meantime, we want each sub-image contains fair amount of pixels of clouds and surfaces to achieve better performances. Here, we introduce two approaches to create the blocks.

The first one is inspired by the nature of K-means clustering. 

```{r}
#From now on, we drop the samples that are not labelled.
image1.valid <- image1%>%filter(label!=0)%>%mutate(image=1)
image2.valid <- image2%>%filter(label!=0)%>%mutate(image=2)
image3.valid <- image3%>%filter(label!=0)%>%mutate(image=3)
```


```{r}
set.seed(1223)
km.out1.bad <- kmeans(image1.valid%>%.[c(4,5,6)],5) # bad result
km.out1 <- kmeans(image1.valid%>%.[c(1,2)],5) # good result, hence we use x,y label to split the data
km.out2 <- kmeans(image2.valid[c(1,2)],5)
km.out3 <- kmeans(image3.valid[c(1,2)],5)
```

```{r}
# bad data split
image1.valid %>%
  mutate(class=km.out1.bad$cluster) %>%
  ggplot(aes(x,y)) +
  geom_raster(aes(fill=class),alpha=0.8) +
  scale_fill_gradientn(colours = c('red','green','blue','yellow','purple'))

# good data split
p21 <- image1.valid %>%
  mutate(class=km.out1$cluster) %>%
  ggplot(aes(x,y)) +
  geom_raster(aes(fill=class),alpha=0.8) +
  scale_fill_gradientn(colours = c('red','green','blue','yellow','purple'))

p22 <- image2.valid %>%
  mutate(class=km.out2$cluster) %>%
  ggplot(aes(x,y)) +
  geom_raster(aes(fill=class),alpha=0.8) +
  scale_fill_gradientn(colours = c('red','green','blue','yellow','purple'))

p23 <- image3.valid %>%
  mutate(class=km.out3$cluster) %>%
  ggplot(aes(x,y)) +
  geom_raster(aes(fill=class),alpha=0.8) +
  scale_fill_gradientn(colours = c('red','green','blue','yellow','purple'))

p21+p22+p23
```
Remember, we want each sub-image contains fair amount of pixels of clouds and surfaces.

3 images * 5 classes/image = 15 classes. We first randomly split train-val-test at a 9-3-3 ratio.

Some statistics: 200000 approx. valid data: 80000 with label 1 and 120000 with label -1. 
Ideally, 120000 training: 50000 with 1 and 70000 with label -1
          40000 val: 16000 with label 1 and 2400 with label -1
          40000 test: 16000 with label 1 and 2400 with label -1
```{r}
rbind(image1.valid %>%
  mutate(class=km.out1$cluster),
  image2.valid %>%
  mutate(class=km.out2$cluster),
  image3.valid %>%
  mutate(class=km.out3$cluster)) %>%
  group_by(image,class,label) %>%
  summarise(n=n())

```

Validation: (1,3), (2,1), (2,5) -1/1=28514/14740
Testing: (1,1), (2,3), (3,5) -1/1=26568/15676
Training: 71998/50565

```{r}
image.val1 <- rbind(image1.valid %>%
  mutate(class=km.out1$cluster)%>%filter(class==3),
  image2.valid %>%
  mutate(class=km.out2$cluster)%>%filter(class %in% c(1,5)))

image.test1 <- rbind(image1.valid %>%
  mutate(class=km.out1$cluster)%>%filter(class==1),
  image2.valid %>%
  mutate(class=km.out2$cluster)%>%filter(class==3),
  image3.valid %>%
  mutate(class=km.out3$cluster)%>%filter(class==5))

image.train1 <- rbind(image1.valid %>%
  mutate(class=km.out1$cluster)%>%filter(!(class %in% c(1,3))),
  image2.valid %>%
  mutate(class=km.out2$cluster)%>%filter(!(class %in% c(1,3,5))),
  image3.valid %>%
  mutate(class=km.out3$cluster)%>%filter(class!=5))

```
The second one: vertical split. For each image, we simply divide the image into 5 sub-images vertically, each of which contains one-fifth pixels of the whole image. Then we look into all the 15 sub-images and assign them into training, validation and testing set.

```{r}
quantile1 <- round(as.numeric(quantile(seq_len(nrow(image1.valid)),c(0,0.2,0.4,0.6,0.8,1))))
quantile2 <- round(as.numeric(quantile(seq_len(nrow(image2.valid)),c(0,0.2,0.4,0.6,0.8,1))))
quantile3 <- round(as.numeric(quantile(seq_len(nrow(image3.valid)),c(0,0.2,0.4,0.6,0.8,1))))

label1 <- c(rep(1,quantile1[2]-quantile1[1]),
            rep(2,quantile1[3]-quantile1[2]),
            rep(3,quantile1[4]-quantile1[3]),
            rep(4,quantile1[5]-quantile1[4]),
            rep(5,quantile1[6]-quantile1[5]+1))
label2 <- c(rep(1,quantile2[2]-quantile2[1]),
            rep(2,quantile2[3]-quantile2[2]),
            rep(3,quantile2[4]-quantile2[3]),
            rep(4,quantile2[5]-quantile2[4]),
            rep(5,quantile2[6]-quantile2[5]+1))
label3 <- c(rep(1,quantile3[2]-quantile3[1]),
            rep(2,quantile3[3]-quantile3[2]),
            rep(3,quantile3[4]-quantile3[3]),
            rep(4,quantile3[5]-quantile3[4]),
            rep(5,quantile3[6]-quantile3[5]+1))

rbind(image1.valid %>%
  mutate(class=label1),
  image2.valid %>%
  mutate(class=label2),
  image3.valid %>%
  mutate(class=label3)) %>%
  group_by(image,class,label) %>%
  summarise(n=n())

rbind(image1.valid %>%
  mutate(class=label1),
  image2.valid %>%
  mutate(class=label2),
  image3.valid %>%
  mutate(class=label3)) %>%
  group_by(label) %>%
  summarise(n=n())

```

Validation: (1,2), (2,2), (2,5) -1/1=25214/19583
Testing: (1,1), (2,3), (3,4) -1/1=24684/16928
Training: 77182/44470

```{r}
image.val2 <- rbind(image1.valid %>%
  mutate(class=label1)%>%filter(class==2),
  image2.valid %>%
  mutate(class=label2)%>%filter(class %in% c(2,5)))

image.test2 <- rbind(image1.valid %>%
  mutate(class=label1)%>%filter(class==1),
  image2.valid %>%
  mutate(class=label2)%>%filter(class==3),
  image3.valid %>%
  mutate(class=label3)%>%filter(class==4))

image.train2 <- rbind(image1.valid %>%
  mutate(class=label1)%>%filter(!(class %in% c(1,2))),
  image2.valid %>%
  mutate(class=label2)%>%filter(!(class %in% c(2,3,5))),
  image3.valid %>%
  mutate(class=label3)%>%filter(class!=4))

```

### (b)

```{r}
mean(image.val1$label==rep(-1,nrow(image.val1)))
mean(image.val2$label==rep(-1,nrow(image.val2)))
mean(image.test1$label==rep(-1,nrow(image.test1)))
mean(image.test2$label==rep(-1,nrow(image.test2)))
```

Accuracy is around 60%. It is trivial that this classifier has a high accuracy when there is almost no cloud in the image.

### (c)

```{r}

detect_difference <- function(x,v1,v2){
  mean(v1<x)+mean(v2>x)
}

differences <- c()

for (i in seq(4,11,1)){
  min.max <- as.numeric(quantile(image1.valid[,i],c(0,1))) # return min and max of a feature
  
  differences <- c(differences,
                   max(
                     max(sapply(seq(min.max[1],min.max[2],length.out=1000),
                                detect_difference,
                                v1=image1.valid%>%filter(label==-1)%>%.[i]%>%unlist(),
                                v2=image1.valid%>%filter(label==1)%>%.[i]%>%unlist())),
                     max(sapply(seq(min.max[1],min.max[2],length.out=1000),
                                detect_difference,
                                v1=image1.valid%>%filter(label==1)%>%.[i]%>%unlist(),
                                v2=image1.valid%>%filter(label==-1)%>%.[i]%>%unlist()))
                     )
                   )
}

differences

```

Therefore, the first three features incorporate more information. 


### (d)

```{r}
# Write a generic cross validation (CV) function CVmaster in R that takes a generic classifier, training features, training labels, number of folds K and a loss function (at least classification accuracy should be there) as inputs and outputs the K-fold CV loss on the training set. Please remember to put it in your github folder in Section 5.

# combine all images
image1 = image1 %>% mutate(ID = 1)
image2 = image2 %>% mutate(ID = 2)
image3 = image3 %>% mutate(ID = 3)
image = rbind(image1, image2, image3)

# separate the features to three images based on image ID
separateImage = function(data){
  # extract ID
  ID = data %>% distinct(ID)
  image = list()
  for (i in seq_along(ID)){
    image[[i]] = data %>% select(ID == i)
  }
  return (image)
}

# split method 1: Kmeans
folds.Kmean = function(features, labels, K){
  # this is stupid
  data = cbind(features, labels)
  
  # get the list of image
  image = separateImage(data)
  
  # divide to K components for each image
  # data with fold (class) indicator
  fold = data.frame()
  for (i in image){
    km.out = kmeans(i[c("y", "x")], K)$cluster
    fold = rbind(fold, i %>% mutate(class = km.out))
  }
  
  # return a list of K folds
  return (fold)
}

# split method 2: vertical split
folds.Vsplit = function(features, labels, K){
  # again, this is stupid
  data = cbind(features, labels)
  
  # get the list of image
  image = separateImage(data)
  
  # divide to K components for each image
  # data with fold (class) indicator
  fold = data.frame()
  for (i in image){
    quantile = round(as.numeric(quantile(seq_len(nrow(i)), seq(0, 1, 1/K))))
    classinfo = c()
    for(k in 1:K){
      if (k != K)
        classinfo = c(classinfo, rep(k, quantile[k + 1] - quantile[k]))
      else
        classinfo = c(classinfo, rep(k, quantile[k + 1] - quantile[k] + 1))
    }
    fold = rbind(fold, cbind(i, class = classinfo))
  }
  
  # return a list of K folds
  return (fold)   
}

#' takes a generic classifier, training features, training labels, number of folds K and a loss function and output the K-fold CV loss on the training set
#' 
#' @param classifier 
#' @param features A data frame containing of all the features (with column names). Needs to include image ID (1, 2, 3) as identifier and x, y axes.
#' @param labels A vector of labels. One-to-one correspondence with the features
#' @param K An integer. The number of folds.
#' @param loss A function that takes two inputs of data frames/vectors (classified labels and predicted labels)
#' @returns A number: the K-fold CV loss on the training set
#' @examples
#' CVmaster(linear_reg, image.features, image.labels, 5, roc_auc)
CVmaster = function(classifier, features, labels, K, loss, split_function){
  
  # get K folds
  folds = split_function(features, labels, K)
  
  # return the K-fold CV loss on the training set
}
```

























