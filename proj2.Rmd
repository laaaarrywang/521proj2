---
title: "STA521 PROJECT 2"
author: "Linxuan Wang, Jingan Zhou"
date: "11/28/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,cache=T)
library(tidyverse)
library(ggplot2)
library(GGally)
library(patchwork)
library(stats)
```

## Data loading
```{r, results='hide'}
image1 = read.table("image_data/imagem1.txt", header=F)
colnames(image1) = c("y","x","label","NDAI","SD","CORR","DF","CF","BF","AF","AN")
image2 = read.table("image_data/imagem2.txt", header=F)
colnames(image2) = c("y","x","label","NDAI","SD","CORR","DF","CF","BF","AF","AN")
image3 = read.table("image_data/imagem3.txt", header=F)
colnames(image3) = c("y","x","label","NDAI","SD","CORR","DF","CF","BF","AF","AN")
```

## Question 1
### (a) half-page summary

### (b) summary and plot
```{r, summary of the data}

image1 %>%
  group_by(label) %>%
  summarise(count = n())

image2 %>%
  group_by(label) %>%
  summarise(count = n())

image3 %>%
  group_by(label) %>%
  summarise(count = n())

```
```{r, plot}
image1 %>%
  ggplot(aes(x,y)) +
  geom_raster(aes(fill=label)) +
  scale_fill_gradientn(colours = c('grey','black','white'))

image2 %>%
  ggplot(aes(x,y)) +
  geom_raster(aes(fill=label)) +
  scale_fill_gradientn(colours = c('grey','black','white'))

image3 %>%
  ggplot(aes(x,y)) +
  geom_raster(aes(fill=label)) +
  scale_fill_gradientn(colours = c('grey','black','white'))
```
We observe several trends and patterns: (i) Areas of both clouds and surfaces tend to be large. In other words, there is almost no slim slices of clouds and surfaces. This is reflected in the dataset as pixels near to each other tend to have the same labels, unless they are at the boundaries between classes with different labels; (ii) Almost no "black holes" exist in either clouds or surfaces. This pattern shows somewhat continuity of cloud and surface distribution: 
 
The i.i.d. assumption is not justified for the samples. Since we have observed large areas of clouds and surfaces, given a pixel in the image, in most of the scenarios it has the same label as its neighbors, which implies that pixels near each other are highly dependent and tend to have the same label.

### (c) Visual and quantitative EDA
#### (i)
```{r,cache=TRUE, pairwise relationship}
image_binded <- rbind(image1,image2,image3)
ggpairs(image_binded,columns = c("NDAI", "SD", "CORR","DF","CF","BF","AF","AN"))
```

#### (ii)
```{r}
p1 = image_binded %>% mutate(label = as.character(label)) %>%
  ggplot() +
  geom_density(aes(x=SD, fill=label),alpha=0.6)

p2 = image_binded %>% mutate(label = as.character(label),
                        log_SD = log(SD)) %>%
  ggplot() +
  geom_density(aes(x=log_SD, fill=label),alpha=0.6)

p3 = image_binded %>% mutate(label = as.character(label)) %>%
  ggplot() +
  geom_density(aes(x=NDAI, fill=label),alpha=0.6)

p4 = image_binded %>% mutate(label = as.character(label)) %>%
  ggplot() +
  geom_density(aes(x=CORR, fill=label),alpha=0.6)

p5 = image_binded %>% mutate(label = as.character(label)) %>%
  ggplot() +
  geom_density(aes(x=DF, fill=label),alpha=0.6)

p6 = image_binded %>% mutate(label = as.character(label)) %>%
  ggplot() +
  geom_density(aes(x=CF, fill=label),alpha=0.6)

p7 = image_binded %>% mutate(label = as.character(label)) %>%
  ggplot() +
  geom_density(aes(x=BF, fill=label),alpha=0.6)

p8 = image_binded %>% mutate(label = as.character(label)) %>%
  ggplot() +
  geom_density(aes(x=AF, fill=label),alpha=0.6)

p9 = image_binded %>% mutate(label = as.character(label)) %>%
  ggplot() +
  geom_density(aes(x=AN, fill=label),alpha=0.6)

p1+p2+p3+p4

p5+p6+p7+p8+p9
```
For the three essential engineered features (SD,NDAI,CORR): samples labelled "no cloud" typically have lower SD, NDAI and CORR. 

For the five radience features: samples labelled "no cloud" typically have two peaks, and are more tightly distributed.
samples labelled "cloud" are more sparsely distributed and negatively skewed. 


## Question 2

### (a)
Ideally, we want our training and validation sets to be as different as possible since the real pictures can be really heterogeneous. If we just use random number generator to specify the samples that should be thrown to the training and validation set respectively, the training "image" and validation "image" will intersect with each other severely. High degree of intersection leads to similarity between training and validation images because samples that are near to each other tend to have same labels. In that case, the validation error is not a good proxy for testing error, and thus testing error will possibly be much higher than validation error when we apply the classification model on a new image.

To take the fact that data is not i.i.d. into account, we would like to split the image into several "blocks" that are treated as sub-images, such that each block only borders other blocks at the boundary. This data splitting rule significantly reduces the similarity between training set and validation set as well as the testing set. In the meantime, we want each sub-image contains fair size of clouds and surfaces to achieve better performances. Here, we introduce two approaches to create the blocks.

The first one is imspired by the nature of K-means clustering. 

```{r}
#From now on, we drop the samples that are not labelled.
image1.valid <- image1%>%filter(label!=0)%>%mutate(image=1)
image2.valid <- image2%>%filter(label!=0)%>%mutate(image=2)
image3.valid <- image3%>%filter(label!=0)%>%mutate(image=3)
```


```{r}
set.seed(1223)
km.out1.bad <- kmeans(image1.valid%>%.[c(4,5,6)],5) # bad result
km.out1 <- kmeans(image1.valid%>%.[c(1,2)],5) # good result, hence we use x,y label to split the data
km.out2 <- kmeans(image2.valid[c(1,2)],5)
km.out3 <- kmeans(image3.valid[c(1,2)],5)
```

```{r}
# bad data split
image1.valid %>%
  mutate(class=km.out1.bad$cluster) %>%
  ggplot(aes(x,y)) +
  geom_raster(aes(fill=class),alpha=0.8) +
  scale_fill_gradientn(colours = c('red','green','blue','yellow','purple'))

# good data split
p21 <- image1.valid %>%
  mutate(class=km.out1$cluster) %>%
  ggplot(aes(x,y)) +
  geom_raster(aes(fill=class),alpha=0.8) +
  scale_fill_gradientn(colours = c('red','green','blue','yellow','purple'))

p22 <- image2.valid %>%
  mutate(class=km.out2$cluster) %>%
  ggplot(aes(x,y)) +
  geom_raster(aes(fill=class),alpha=0.8) +
  scale_fill_gradientn(colours = c('red','green','blue','yellow','purple'))

p23 <- image3.valid %>%
  mutate(class=km.out3$cluster) %>%
  ggplot(aes(x,y)) +
  geom_raster(aes(fill=class),alpha=0.8) +
  scale_fill_gradientn(colours = c('red','green','blue','yellow','purple'))

p21+p22+p23
```
Remember, we want each sub-image contains fair amount of clouds and surfaces.

3 images * 5 classes/image = 15 classes. We first randomly split train-val-test at a 9-3-3 ratio.

Some statistics: 200000 approx. valid data: 80000 with label 1 and 120000 with label -1. 
Ideally, 120000 training: 50000 with 1 and 70000 with label -1
          40000 val: 16000 with label 1 and 2400 with label -1
          40000 test: 16000 with label 1 and 2400 with label -1
```{r}
rbind(image1.valid %>%
  mutate(class=km.out1$cluster),
  image2.valid %>%
  mutate(class=km.out2$cluster),
  image3.valid %>%
  mutate(class=km.out3$cluster)) %>%
  group_by(image,class,label) %>%
  summarise(n=n())

```

Validation: (1,3), (2,1), (2,5) -1/1=28514/14740
Testing: (1,1), (2,3), (3,5) -1/1=26568/15676
Training: 71998/50565

```{r}
image.val1 <- rbind(image1.valid %>%
  mutate(class=km.out1$cluster)%>%filter(class==3),
  image2.valid %>%
  mutate(class=km.out2$cluster)%>%filter(class %in% c(1,5)))

image.test1 <- rbind(image1.valid %>%
  mutate(class=km.out1$cluster)%>%filter(class==1),
  image2.valid %>%
  mutate(class=km.out2$cluster)%>%filter(class==3),
  image3.valid %>%
  mutate(class=km.out3$cluster)%>%filter(class==5))

image.train1 <- rbind(image1.valid %>%
  mutate(class=km.out1$cluster)%>%filter(!(class %in% c(1,3))),
  image2.valid %>%
  mutate(class=km.out2$cluster)%>%filter(!(class %in% c(1,3,5))),
  image3.valid %>%
  mutate(class=km.out3$cluster)%>%filter(class!=5))

```
The second one: vertical split. For each image, we simply divide the image into 5 sub-images vertically, each of which contains one-fifth pixels of the whole image. Then we look into all the 15 sub-images and assign them into training, validation and testing set.

```{r}
quantile1 <- round(as.numeric(quantile(seq_len(nrow(image1.valid)),c(0,0.2,0.4,0.6,0.8,1))))
quantile2 <- round(as.numeric(quantile(seq_len(nrow(image2.valid)),c(0,0.2,0.4,0.6,0.8,1))))
quantile3 <- round(as.numeric(quantile(seq_len(nrow(image3.valid)),c(0,0.2,0.4,0.6,0.8,1))))

label1 <- c(rep(1,quantile1[2]-quantile1[1]),
            rep(2,quantile1[3]-quantile1[2]),
            rep(3,quantile1[4]-quantile1[3]),
            rep(4,quantile1[5]-quantile1[4]),
            rep(5,quantile1[6]-quantile1[5]+1))
label2 <- c(rep(1,quantile2[2]-quantile2[1]),
            rep(2,quantile2[3]-quantile2[2]),
            rep(3,quantile2[4]-quantile2[3]),
            rep(4,quantile2[5]-quantile2[4]),
            rep(5,quantile2[6]-quantile2[5]+1))
label3 <- c(rep(1,quantile3[2]-quantile3[1]),
            rep(2,quantile3[3]-quantile3[2]),
            rep(3,quantile3[4]-quantile3[3]),
            rep(4,quantile3[5]-quantile3[4]),
            rep(5,quantile3[6]-quantile3[5]+1))

rbind(image1.valid %>%
  mutate(class=label1),
  image2.valid %>%
  mutate(class=label2),
  image3.valid %>%
  mutate(class=label3)) %>%
  group_by(image,class,label) %>%
  summarise(n=n())

rbind(image1.valid %>%
  mutate(class=label1),
  image2.valid %>%
  mutate(class=label2),
  image3.valid %>%
  mutate(class=label3)) %>%
  group_by(label) %>%
  summarise(n=n())

```

Validation: (1,2), (2,2), (2,5) -1/1=25214/19583
Testing: (1,1), (2,3), (3,4) -1/1=24684/16928
Training: 77182/44470

```{r}
image.val2 <- rbind(image1.valid %>%
  mutate(class=label1)%>%filter(class==2),
  image2.valid %>%
  mutate(class=label2)%>%filter(class %in% c(2,5)))

image.test2 <- rbind(image1.valid %>%
  mutate(class=label1)%>%filter(class==1),
  image2.valid %>%
  mutate(class=label2)%>%filter(class==3),
  image3.valid %>%
  mutate(class=label3)%>%filter(class==4))

image.train2 <- rbind(image1.valid %>%
  mutate(class=label1)%>%filter(!(class %in% c(1,2))),
  image2.valid %>%
  mutate(class=label2)%>%filter(!(class %in% c(2,3,5))),
  image3.valid %>%
  mutate(class=label3)%>%filter(class!=4))

```

### (b)

```{r}
mean(image.val1$label==rep(-1,nrow(image.val1)))
mean(image.val2$label==rep(-1,nrow(image.val2)))
mean(image.test1$label==rep(-1,nrow(image.test1)))
mean(image.test2$label==rep(-1,nrow(image.test2)))
```

Accuracy is around 60%. It is trivial that this classifier has a high accuracy when there is almost no cloud in the image.

### (c)






























